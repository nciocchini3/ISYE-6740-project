{"cells":[{"cell_type":"code","source":["# install libraries not included\n","!pip install catboost\n","!pip install bayesian-optimization"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sRBDFUdYY-OO","executionInfo":{"status":"ok","timestamp":1764386814680,"user_tz":420,"elapsed":13164,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}},"outputId":"9aedb40c-56cc-4484-e20a-51b809ddb1b7"},"id":"sRBDFUdYY-OO","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting catboost\n","  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n","Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.8\n","Collecting bayesian-optimization\n","  Downloading bayesian_optimization-3.1.0-py3-none-any.whl.metadata (11 kB)\n","Collecting colorama>=0.4.6 (from bayesian-optimization)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (2.0.2)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (1.6.1)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->bayesian-optimization) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->bayesian-optimization) (3.6.0)\n","Downloading bayesian_optimization-3.1.0-py3-none-any.whl (36 kB)\n","Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Installing collected packages: colorama, bayesian-optimization\n","Successfully installed bayesian-optimization-3.1.0 colorama-0.4.6\n"]}]},{"cell_type":"code","execution_count":2,"id":"0c072d24","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-01-31T23:23:31.452925Z","iopub.status.busy":"2025-01-31T23:23:31.452255Z","iopub.status.idle":"2025-01-31T23:23:32.675395Z","shell.execute_reply":"2025-01-31T23:23:32.673779Z"},"papermill":{"duration":1.238444,"end_time":"2025-01-31T23:23:32.678420","exception":false,"start_time":"2025-01-31T23:23:31.439976","status":"completed"},"tags":[],"id":"0c072d24","executionInfo":{"status":"ok","timestamp":1764386828029,"user_tz":420,"elapsed":13345,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","from google.colab import drive\n","\n","# import libraries\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n","from sklearn.metrics import roc_auc_score\n","from sklearn.decomposition import PCA\n","\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from xgboost import XGBClassifier\n","from sklearn.utils import class_weight\n","from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, SMOTENC\n","from imblearn.combine import SMOTEENN, SMOTETomek\n","\n","from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n","\n","from bayes_opt import BayesianOptimization"]},{"cell_type":"code","source":["# script parameters\n","alpha = 1 # exp smoothing (1 = no smoothing)\n","# data_augmentation = 'none'\n","data_augmentation = 'subsample-majority'\n","# data_augmentation = 'smotenc'\n","# data_augmentation = 'adasyn'"],"metadata":{"id":"12k5HJIQIYfI","executionInfo":{"status":"ok","timestamp":1764387640811,"user_tz":420,"elapsed":7,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}}},"id":"12k5HJIQIYfI","execution_count":18,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive', force_remount=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZ6oeYqvySma","executionInfo":{"status":"ok","timestamp":1764387641668,"user_tz":420,"elapsed":645,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}},"outputId":"621c4e1b-a5a1-4bb0-8b91-ccde43f20d29"},"id":"yZ6oeYqvySma","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# os.chdir('/content/drive/My Drive/ISYE6740_project')\n","\n","# for dirname, _, filenames in os.walk('/content/drive/My Drive/ISYE6740_project/data/'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"id":"U6bJ14HAX7Qw","executionInfo":{"status":"ok","timestamp":1764387641863,"user_tz":420,"elapsed":2,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}}},"id":"U6bJ14HAX7Qw","execution_count":20,"outputs":[]},{"cell_type":"code","execution_count":21,"id":"3770fd87","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:23:47.883592Z","iopub.status.busy":"2025-01-31T23:23:47.882984Z","iopub.status.idle":"2025-01-31T23:23:47.887551Z","shell.execute_reply":"2025-01-31T23:23:47.886082Z"},"papermill":{"duration":0.016986,"end_time":"2025-01-31T23:23:47.889402","exception":false,"start_time":"2025-01-31T23:23:47.872416","status":"completed"},"tags":[],"id":"3770fd87","executionInfo":{"status":"ok","timestamp":1764387642158,"user_tz":420,"elapsed":2,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}}},"outputs":[],"source":["# # debug purpose\n","# person_id_debug = 72083416"]},{"cell_type":"code","execution_count":27,"id":"789e913a","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:23:47.910839Z","iopub.status.busy":"2025-01-31T23:23:47.910470Z","iopub.status.idle":"2025-01-31T23:23:47.946228Z","shell.execute_reply":"2025-01-31T23:23:47.945015Z"},"papermill":{"duration":0.049278,"end_time":"2025-01-31T23:23:47.948299","exception":false,"start_time":"2025-01-31T23:23:47.899021","status":"completed"},"tags":[],"id":"789e913a","executionInfo":{"status":"ok","timestamp":1764387863580,"user_tz":420,"elapsed":25,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}}},"outputs":[],"source":["def process_sepsis_data(folder = 'train'):\n","    path_base = '/content/drive/My Drive/ISYE6740_project/data/'\n","    fname='SepsisLabel_' + folder\n","    if folder == 'train':\n","        sub_path = 'training_data/'\n","    else:\n","        sub_path = 'testing_data/'\n","\n","    fpath = path_base + sub_path + fname + '.csv'\n","\n","    # load data\n","    df = pd.read_csv(fpath)\n","\n","    # convert to datetime\n","    df['measurement_datetime'] = pd.to_datetime(df['measurement_datetime'], errors='coerce')\n","    df['measurement_datetime_hour'] = df['measurement_datetime'].dt.floor('h')\n","\n","    # sort table\n","    # df = df.sort_values(by=['person_id', 'measurement_datetime'])\n","\n","    # if folder == 'train':\n","    #     # create new column with cumulative sum of Sepsis events\n","    #     # (just the first eevent will be considered, as we are not provided information about previous occurence as feature)\n","    #     df['sepsis_hours'] = (df.groupby(['person_id'], as_index=False)['SepsisLabel'].cumsum())\n","\n","        # derivative of sepsis events - not used\n","        # df['sep_diff'] = df['SepsisLabel'].rolling(window=2).apply(np.diff)\n","        # df['sep_diff'] = df['SepsisLabel'].diff()\n","        # df['win_delimeters'] = 0\n","        # df.loc[df['sep_diff'] == 1, 'win_delimeters'] = 1\n","        # df['windows'] = df.groupby(['person_id'], as_index=False)['win_delimeters'].cumsum()\n","\n","        # df = df[df['sepsis_hours']<=1]\n","\n","    return df\n","\n","def process_measure_data(folder = 'train', meas_type = 'lab', alpha=0.2):\n","    path_base = '/content/drive/My Drive/ISYE6740_project/data/'\n","    fname='measurement_'+ meas_type + '_' + folder\n","    if folder == 'train':\n","        sub_path = 'training_data/'\n","    else:\n","        sub_path = 'testing_data/'\n","\n","    fpath = path_base + sub_path + fname + '.csv'\n","\n","    # load data\n","    df = pd.read_csv(fpath)\n","\n","    # convert to datetime\n","    df['measurement_datetime'] = pd.to_datetime(df['measurement_datetime'], errors='coerce')\n","\n","    # map categorical variable for observatiuon table\n","    if meas_type == 'observation':\n","        di = {'Normal capillary filling':0, 'Decreased capillary filling time':-1, 'Increased capillary filling time':1}\n","        df['Capillary refill [Time]'] = df['Capillary refill [Time]'].map(di)\n","\n","        di = {'Absent':-1, 'Present':1, 'Weak':0}\n","        df['Pulse'] = df['Pulse'].map(di)\n","        df['Arterial pulse pressure'] = df['Arterial pulse pressure'].map(di)\n","\n","        di = {'Normal':0, 'Sluggish':1}\n","        df['Right pupil Pupillary response'] = df['Right pupil Pupillary response'].map(di)\n","        df['Left pupil Pupillary response'] = df['Left pupil Pupillary response'].map(di)\n","\n","    # sort table\n","    df = df.sort_values(by=['person_id', 'visit_occurrence_id', 'measurement_datetime'])\n","\n","    df = df.set_index('measurement_datetime')\n","    df = df.groupby(['person_id', 'visit_occurrence_id'], as_index=False).resample('h').asfreq().interpolate(method='linear')\n","\n","    # df[['person_id', 'visit_occurrence_id']] = df[['person_id', 'visit_occurrence_id']].fillna(method='ffill')\n","\n","    df = df.reset_index()\n","\n","    # cols = list(df.columns)\n","    # cols.remove('person_id')\n","    # cols.remove('measurement_datetime')\n","    # cols.remove('visit_occurrence_id')\n","\n","    # df[cols] = df.groupby(['person_id'])[cols].ffill()\n","    # df[cols] = df.groupby(['person_id'])[cols].bfill()\n","\n","    # df = df.fillna(0) # no data imputation possible\n","\n","    # df = df.reset_index()\n","    df.rename(columns = {'measurement_datetime': 'measurement_datetime_hour'}, inplace=True)\n","\n","    df['person_id'] = df['person_id'].astype(int)\n","    df['visit_occurrence_id'] = df['visit_occurrence_id'].astype(int)\n","\n","    # exponential moving average\n","    cols = list(df.columns)\n","    cols.remove('person_id')\n","    cols.remove('measurement_datetime_hour')\n","    cols.remove('visit_occurrence_id')\n","\n","    # df[cols] = df[cols].ewm(alpha=alpha).mean()\n","\n","    for col in cols:\n","        df[col] = df.groupby(['person_id'])[col].ewm(alpha=alpha).mean().values\n","\n","    df = df.reset_index()\n","\n","\n","    # df.drop('level_0', 'visit_occurrence_id'], axis=1, inplace=True)\n","    df.drop(['level_0', 'visit_occurrence_id', 'index'], axis=1, inplace=True)\n","\n","    return df\n","\n","def process_demo_data(folder = 'train', one_hot_encoding=False):\n","    path_base = '/content/drive/My Drive/ISYE6740_project/data/'\n","    fname='person_demographics_episode' + '_' + folder\n","\n","    if folder == 'train':\n","        sub_path = 'training_data/'\n","    else:\n","        sub_path = 'testing_data/'\n","\n","    fpath = path_base + sub_path + fname + '.csv'\n","\n","    # load data\n","    df = pd.read_csv(fpath)\n","\n","    # convert to datetime\n","    # df['measurement_datetime'] = pd.to_datetime(df['measurement_datetime'], errors='coerce')\n","    # df['measurement_datetime_day'] = df['measurement_datetime'].dt.floor('d')\n","\n","    if one_hot_encoding:\n","      # map categorical variable for observatiuon table\n","      df = pd.get_dummies(df, columns=['gender'], dtype=float)\n","      # di = {'MALE':0, 'FEMALE':1}\n","      # df['gender'] = df['gender'].map(di)\n","\n","    df.drop(['visit_occurrence_id', 'visit_start_date', 'birth_datetime'], axis=1, inplace=True)\n","\n","    return df\n","\n","def process_drug_data(folder = 'train'):\n","    path_base = '/content/drive/My Drive/ISYE6740_project/data/'\n","    fname='drugsexposure' + '_' + folder\n","\n","    if folder == 'train':\n","        sub_path = 'training_data/'\n","    else:\n","        sub_path = 'testing_data/'\n","\n","    fpath = path_base + sub_path + fname + '.csv'\n","\n","    # load data\n","    df = pd.read_csv(fpath)\n","\n","    # convert to datetime\n","    df['drug_datetime_hourly'] = pd.to_datetime(df['drug_datetime_hourly'], errors='coerce')\n","\n","    # create new column by concatenating drug_concept_id and route_concept id\n","    df['drug_route'] = df['drug_concept_id'].astype(str) + '_' + df['route_concept_id'].astype(str)\n","\n","    # remove drug_concept_id and route_cocept_id columns\n","    df.drop(['drug_concept_id', 'route_concept_id'], axis=1, inplace=True)\n","\n","\n","\n","    # one-hot encode\n","    df = pd.get_dummies(df, columns=['drug_route'], dtype=float)\n","\n","    # sort table\n","    df = df.sort_values(by=['person_id', 'visit_occurrence_id', 'drug_datetime_hourly'])\n","\n","    df = df.drop_duplicates(subset=['person_id', 'visit_occurrence_id', 'drug_datetime_hourly'])\n","\n","    df = df.set_index(['drug_datetime_hourly'])\n","\n","    # fill rules\n","    cols_sum_sum = list(df.columns)\n","    cols_sum_sum.remove('person_id')\n","    # cols_sum_sum.remove('drug_datetime_hourly')\n","    cols_sum_sum.remove('visit_occurrence_id')\n","\n","\n","    df = df.groupby(['person_id', 'visit_occurrence_id'], as_index=False).resample('h').asfreq()\n","\n","    df[['person_id', 'visit_occurrence_id']] = df[['person_id', 'visit_occurrence_id']].ffill()\n","    df[cols_sum_sum] = df[cols_sum_sum].fillna(0)\n","\n","\n","\n","    # group by person_id and visit_occurence_id cum sum of other columns\n","    df[cols_sum_sum] = df.groupby(['person_id', 'visit_occurrence_id'], as_index=False)[cols_sum_sum].cumsum()\n","\n","\n","    # # df = df.resample('d', on='measurement_datetime').mean()\n","\n","    # df[['person_id', 'visit_occurrence_id']] = df[['person_id', 'visit_occurrence_id']].fillna(method='ffill')\n","\n","    # df = df.fillna(0)\n","\n","    df = df.reset_index()\n","\n","    # cols = list(df.columns)\n","    # cols.remove('person_id')\n","    # cols.remove('drug_datetime_hourly')\n","    # cols.remove('visit_occurrence_id')\n","\n","    # df[cols] = df.groupby(['person_id'])[cols].ffill()\n","    # df[cols] = df.groupby(['person_id'])[cols].bfill()\n","\n","    # df = df.fillna(0) # no data imputation possible\n","\n","    df = df.reset_index()\n","    df.rename(columns = {'drug_datetime_hourly': 'measurement_datetime_hour'}, inplace=True)\n","    df['person_id'] = df['person_id'].astype(int)\n","\n","\n","\n","    # # exponential moving average\n","    cols = list(df.columns)\n","    cols.remove('person_id')\n","    cols.remove('measurement_datetime_hour')\n","    cols.remove('visit_occurrence_id')\n","\n","\n","    for col in cols:\n","        df[col] = df.groupby(['person_id'])[col].ewm(alpha=alpha).mean().values\n","\n","    # df = df.reset_index()\n","\n","\n","    # df.drop(['index', 'level_0', 'visit_occurrence_id'], axis=1, inplace=True)\n","    df.drop(['index', 'level_0', 'visit_occurrence_id'], axis=1, inplace=True)\n","\n","    return df\n","\n","def process_procedure_data(folder = 'train', one_hot_encode=False):\n","    path_base = '/content/drive/My Drive/ISYE6740_project/data/'\n","    fname='proceduresoccurrences' + '_' + folder\n","\n","    if folder == 'train':\n","        sub_path = 'training_data/'\n","    else:\n","        sub_path = 'testing_data/'\n","\n","    fpath = path_base + sub_path + fname + '.csv'\n","\n","    # load data\n","    df = pd.read_csv(fpath)\n","\n","    # convert to datetime\n","    df['procedure_datetime_hourly'] = pd.to_datetime(df['procedure_datetime_hourly'], errors='coerce')\n","\n","    # one-hot encode\n","    if one_hot_encode:\n","      df = pd.get_dummies(df, columns=['procedure'], dtype=float)\n","\n","    # sort table\n","    df = df.sort_values(by=['person_id', 'visit_occurrence_id', 'procedure_datetime_hourly'])\n","\n","    df = df.drop_duplicates(subset=['person_id', 'visit_occurrence_id', 'procedure_datetime_hourly'])\n","\n","    df = df.set_index(['procedure_datetime_hourly'])\n","\n","    df = df.groupby(['person_id', 'visit_occurrence_id'], as_index=False).resample('h').asfreq().fillna(method='ffill')\n","    # # df = df.resample('d', on='measurement_datetime').mean()\n","\n","    # df[['person_id', 'visit_occurrence_id']] = df[['person_id', 'visit_occurrence_id']].fillna(method='ffill')\n","\n","    df = df.reset_index()\n","\n","    # cols = list(df.columns)\n","    # cols.remove('person_id')\n","    # cols.remove('procedure_datetime_hourly')\n","    # cols.remove('visit_occurrence_id')\n","\n","    # df[cols] = df.groupby(['person_id'])[cols].ffill()\n","    # df[cols] = df.groupby(['person_id'])[cols].bfill()\n","    # df = df.fillna(0) # no data imputation possible\n","\n","    # df = df.reset_index()\n","\n","    df.rename(columns = {'procedure_datetime_hourly': 'measurement_datetime_hour'}, inplace=True)\n","\n","    # # # exponential moving average\n","    # cols = list(df.columns)\n","    # cols.remove('person_id')\n","    # cols.remove('measurement_datetime_hour')\n","    # cols.remove('visit_occurrence_id')\n","\n","    # # # df[cols] = df[cols].ewm(alpha=alpha).mean()\n","\n","    # for col in cols:\n","    #     df[col] = df.groupby(['person_id'])[col].ewm(alpha=alpha).mean().values\n","\n","    # df = df.reset_index()\n","\n","    df['person_id'] = df['person_id'].astype(int)\n","\n","    df.drop([ 'level_0', 'visit_occurrence_id'], axis=1, inplace=True)\n","\n","    return df\n","\n","def process_device_data(folder = 'train', one_hot_encode=False):\n","    path_base = '/content/drive/My Drive/ISYE6740_project/data/'\n","    fname='devices' + '_' + folder\n","\n","    if folder == 'train':\n","        sub_path = 'training_data/'\n","    else:\n","        sub_path = 'testing_data/'\n","\n","    fpath = path_base + sub_path + fname + '.csv'\n","\n","    # load data\n","    df = pd.read_csv(fpath)\n","\n","    # convert to datetime\n","    df['device_datetime_hourly'] = pd.to_datetime(df['device_datetime_hourly'], errors='coerce')\n","\n","    # one-hot encode\n","    if one_hot_encode:\n","      df = pd.get_dummies(df, columns=['device'], dtype=float)\n","\n","    # sort table\n","    df = df.sort_values(by=['person_id', 'visit_occurrence_id', 'device_datetime_hourly'])\n","\n","    df = df.drop_duplicates(subset=['person_id', 'visit_occurrence_id', 'device_datetime_hourly'])\n","\n","    df = df.set_index(['device_datetime_hourly'])\n","\n","    df = df.groupby(['person_id', 'visit_occurrence_id'], as_index=False).resample('h').asfreq().fillna(method='ffill')\n","    # # df = df.resample('d', on='measurement_datetime').mean()\n","\n","\n","    df = df.reset_index()\n","    # cols = list(df.columns)\n","    # cols.remove('person_id')\n","    # cols.remove('device_datetime_hourly')\n","    # cols.remove('visit_occurrence_id')\n","\n","    # df[cols] = df.groupby(['person_id'])[cols].ffill()\n","    # df[cols] = df.groupby(['person_id'])[cols].bfill()\n","    # df = df.fillna(0) # no data imputation possible\n","\n","    # df = df.reset_index()\n","    df.rename(columns = {'device_datetime_hourly': 'measurement_datetime_hour'}, inplace=True)\n","    df['person_id'] = df['person_id'].astype(int)\n","\n","    # # # exponential moving average\n","    # cols = list(df.columns)\n","    # cols.remove('person_id')\n","    # cols.remove('measurement_datetime_hour')\n","    # cols.remove('visit_occurrence_id')\n","\n","    # # # df[cols] = df[cols].ewm(alpha=alpha).mean()\n","\n","    # for col in cols:\n","    #     df[col] = df.groupby(['person_id'])[col].ewm(alpha=alpha).mean().values\n","\n","    # df = df.reset_index()\n","\n","\n","    df.drop(['level_0', 'visit_occurrence_id'], axis=1, inplace=True)\n","\n","    return df\n","\n","def process_obs_data(folder = 'train', one_hot_encode=False):\n","    path_base = '/content/drive/My Drive/ISYE6740_project/data/'\n","    fname='observation' + '_' + folder\n","\n","    if folder == 'train':\n","        sub_path = 'training_data/'\n","    else:\n","        sub_path = 'testing_data/'\n","\n","    fpath = path_base + sub_path + fname + '.csv'\n","\n","    # load data\n","    df = pd.read_csv(fpath)\n","\n","    # convert to datetime\n","    df['observation_datetime'] = pd.to_datetime(df['observation_datetime'], errors='coerce')\n","\n","    # one-hot encode\n","    if one_hot_encode:\n","      df = pd.get_dummies(df, columns=['valuefilled'], dtype=float)\n","\n","    # sort table\n","    df = df.sort_values(by=['person_id', 'visit_occurrence_id', 'observation_datetime'])\n","\n","    df = df.drop_duplicates(subset=['person_id', 'visit_occurrence_id', 'observation_datetime'])\n","\n","    df = df.set_index(['observation_datetime'])\n","\n","    df = df.groupby(['person_id', 'visit_occurrence_id'], as_index=False).resample('h').asfreq().fillna(method='ffill')\n","    # # df = df.resample('d', on='measurement_datetime').mean()\n","\n","    # df = df.fillna(method='ffill')\n","\n","    df = df.reset_index()\n","    # cols = list(df.columns)\n","    # cols.remove('person_id')\n","    # cols.remove('observation_datetime')\n","    # cols.remove('visit_occurrence_id')\n","\n","    # df[cols] = df.groupby(['person_id'])[cols].ffill()\n","    # df[cols] = df.groupby(['person_id'])[cols].bfill()\n","    # df = df.fillna(0) # no data imputation possible\n","\n","    # df = df.reset_index()\n","    df.rename(columns = {'observation_datetime': 'measurement_datetime_hour', 'valuefilled': 'admission_reason'}, inplace=True)\n","    df['person_id'] = df['person_id'].astype(int)\n","\n","    # # # exponential moving average\n","    # cols = list(df.columns)\n","    # cols.remove('person_id')\n","    # cols.remove('measurement_datetime_hour')\n","    # cols.remove('visit_occurrence_id')\n","    # cols.remove('observation_concept_id')\n","    # cols.remove('observation_concept_name')\n","\n","    # # # df[cols] = df[cols].ewm(alpha=alpha).mean()\n","\n","    # for col in cols:\n","    #     df[col] = df.groupby(['person_id'])[col].ewm(alpha=alpha).mean().values\n","\n","    # df = df.reset_index()\n","\n","\n","    df.drop(['level_0', 'visit_occurrence_id', 'observation_concept_id', 'observation_concept_name'], axis=1, inplace=True)\n","\n","    return df\n","\n","\n","def merge_with_main_table(main, sub, folder = 'train', keys=['person_id', 'measurement_datetime_hour']):\n","    main = main.merge(sub.drop_duplicates(subset=keys), on=keys, how='left')\n","\n","    main = main.sort_values(by=['person_id', 'measurement_datetime_hour'])\n","\n","    # main = main.reset_index()\n","\n","    cols = list(main.columns)\n","    cols.remove('person_id')\n","    cols.remove('measurement_datetime')\n","    cols.remove('measurement_datetime_hour')\n","\n","    # cols.remove('visit_occurrence_id')\n","    if folder == 'train':\n","        cols.remove('SepsisLabel')\n","\n","    # main[cols] = main.groupby(['person_id'], as_index=False)[cols].\\\n","    #     transform(lambda x: x.fillna(x.mean()))\n","\n","    # main[cols] = main.groupby(['person_id'])[cols].ffill()\n","    # main[cols] = main.groupby(['person_id'])[cols].bfill()\n","\n","    # main[cols] = main[cols].fillna(0) # no data imputation possible\n","\n","    return main"]},{"cell_type":"code","execution_count":30,"id":"e0260b06","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:23:47.999285Z","iopub.status.busy":"2025-01-31T23:23:47.998815Z","iopub.status.idle":"2025-01-31T23:25:00.495321Z","shell.execute_reply":"2025-01-31T23:25:00.493916Z"},"papermill":{"duration":72.510496,"end_time":"2025-01-31T23:25:00.497644","exception":false,"start_time":"2025-01-31T23:23:47.987148","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"e0260b06","executionInfo":{"status":"ok","timestamp":1764388001092,"user_tz":420,"elapsed":73064,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}},"outputId":"6388e9ed-dcb0-4955-a38d-0ceb21bf8e80"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2084436765.py:262: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df = df.groupby(['person_id', 'visit_occurrence_id'], as_index=False).resample('h').asfreq().fillna(method='ffill')\n","/tmp/ipython-input-2084436765.py:329: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df = df.groupby(['person_id', 'visit_occurrence_id'], as_index=False).resample('h').asfreq().fillna(method='ffill')\n","/tmp/ipython-input-2084436765.py:393: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df = df.groupby(['person_id', 'visit_occurrence_id'], as_index=False).resample('h').asfreq().fillna(method='ffill')\n"]}],"source":["# smoothing average alpha\n","\n","train_df = process_sepsis_data('train')\n","\n","# train_df_sub.tail(10)\n","# train_df_sub.info()\n","# train_df.info()\n","# train_df_sub.to_csv('train_df_sub.csv')\n","meas_lab_train_df = process_measure_data('train', meas_type='lab', alpha=alpha)\n","meas_meds_train_df = process_measure_data('train', meas_type='meds', alpha=alpha)\n","meas_obs_train_df = process_measure_data('train', meas_type='observation', alpha=alpha)\n","\n","demo_train_df = process_demo_data('train')\n","\n","drugs_train_df = process_drug_data('train')\n","\n","procedure_train_df = process_procedure_data('train')\n","\n","device_train_df = process_device_data('train')\n","\n","obs_train_df = process_obs_data('train')\n","\n","# meas_lab_train_df_sub = meas_lab_train_df[meas_lab_train_df['person_id'] == person_id_debug]\n","# train_df_sub.head(50)\n","\n","# merge tables\n","train_df = merge_with_main_table(folder = 'train', main=train_df, sub = meas_lab_train_df, keys=['person_id', 'measurement_datetime_hour'])\n","train_df = merge_with_main_table(folder = 'train', main=train_df, sub = meas_meds_train_df, keys=['person_id', 'measurement_datetime_hour'])\n","train_df = merge_with_main_table(folder = 'train', main=train_df, sub = meas_obs_train_df, keys=['person_id', 'measurement_datetime_hour'])\n","\n","train_df = merge_with_main_table(folder = 'train', main=train_df, sub = demo_train_df, keys=['person_id'])\n","\n","train_df = merge_with_main_table(folder = 'train', main=train_df, sub = drugs_train_df, keys=['person_id', 'measurement_datetime_hour'])\n","\n","train_df = merge_with_main_table(folder = 'train', main=train_df, sub = procedure_train_df, keys=['person_id', 'measurement_datetime_hour'])\n","\n","train_df = merge_with_main_table(folder = 'train', main=train_df, sub = device_train_df, keys=['person_id', 'measurement_datetime_hour'])\n","\n","train_df = merge_with_main_table(folder = 'train', main=train_df, sub = obs_train_df, keys=['person_id', 'measurement_datetime_hour'])\n","\n","# # train_df_sub = train_df[train_df['person_id'] == person_id_debug]\n","# # train_df_sub = train_df[train_df['person_id'] == person_id_debug]\n","\n","# train_df[['route_concept_id_Intraperitoneal', 'route_concept_id_Intratracheal']]=0\n","\n","# train_df['measurement_datetime'] = train_df['measurement_datetime'].astype(str)\n","# train_df['person_id_datetime'] = train_df['person_id'].str.cat(submission['measurement_datetime'], sep='_')\n","\n","# train_df = train_df.set_index(['measurement_datetime'])\n","# train_df = train_df.groupby(['person_id'], as_index=False).resample('h').asfreq().interpolate()\n","\n","# train_df = train_df.reset_index()\n","cols = list(train_df.columns)\n","cols.remove('person_id')\n","cols.remove('measurement_datetime')\n","cols.remove('measurement_datetime_hour')\n","\n","train_df[cols] = train_df.groupby(['person_id'])[cols].ffill()\n","train_df[cols] = train_df.groupby(['person_id'])[cols].bfill()\n","train_df[['gender', 'procedure', 'device', 'admission_reason']] = \\\n","  train_df[['gender', 'procedure', 'device', 'admission_reason']].fillna('unknown')\n","train_df = train_df.fillna(0) # no data imputation possible\n","\n","train_df = train_df.reset_index()\n","\n","train_df.drop(['index'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":32,"id":"819caa22","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:25:03.697297Z","iopub.status.busy":"2025-01-31T23:25:03.696847Z","iopub.status.idle":"2025-01-31T23:25:30.604099Z","shell.execute_reply":"2025-01-31T23:25:30.602338Z"},"papermill":{"duration":26.921114,"end_time":"2025-01-31T23:25:30.606205","exception":false,"start_time":"2025-01-31T23:25:03.685091","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"819caa22","executionInfo":{"status":"ok","timestamp":1764388027384,"user_tz":420,"elapsed":26252,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}},"outputId":"c0d12322-a038-4339-8ef8-1b54fcc0c4ca"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2084436765.py:262: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df = df.groupby(['person_id', 'visit_occurrence_id'], as_index=False).resample('h').asfreq().fillna(method='ffill')\n","/tmp/ipython-input-2084436765.py:329: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df = df.groupby(['person_id', 'visit_occurrence_id'], as_index=False).resample('h').asfreq().fillna(method='ffill')\n","/tmp/ipython-input-2084436765.py:393: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  df = df.groupby(['person_id', 'visit_occurrence_id'], as_index=False).resample('h').asfreq().fillna(method='ffill')\n"]}],"source":["# predict on test dataset\n","# smooething average alpha\n","test_df = process_sepsis_data('test')\n","\n","# train_df_sub.tail(10)\n","# train_df_sub.info()\n","# train_df.info()\n","# train_df_sub.to_csv('train_df_sub.csv')\n","meas_lab_test_df = process_measure_data('test', meas_type='lab', alpha=alpha)\n","meas_meds_test_df = process_measure_data('test', meas_type='meds', alpha=alpha)\n","meas_obs_test_df = process_measure_data('test', meas_type='observation', alpha=alpha)\n","\n","demo_test_df = process_demo_data('test')\n","# meas_lab_train_df_sub = meas_lab_train_df[meas_lab_train_df['person_id'] == person_id_debug]\n","# train_df_sub.head(50)\n","\n","drugs_test_df = process_drug_data('test')\n","\n","procedure_test_df = process_procedure_data('test')\n","\n","device_test_df = process_device_data('test')\n","\n","obs_test_df = process_obs_data('test')\n","\n","# merge tables\n","test_df = merge_with_main_table(folder = 'test', main=test_df, sub = meas_lab_test_df, keys=['person_id', 'measurement_datetime_hour'])\n","test_df = merge_with_main_table(folder = 'test', main=test_df, sub = meas_meds_test_df, keys=['person_id', 'measurement_datetime_hour'])\n","test_df = merge_with_main_table(folder = 'test', main=test_df, sub = meas_obs_test_df, keys=['person_id', 'measurement_datetime_hour'])\n","\n","test_df = merge_with_main_table(folder = 'test', main=test_df, sub = demo_test_df, keys=['person_id'])\n","\n","test_df = merge_with_main_table(folder = 'test', main=test_df, sub = drugs_test_df, keys=['person_id', 'measurement_datetime_hour'])\n","\n","test_df = merge_with_main_table(folder = 'test', main=test_df, sub = procedure_test_df, keys=['person_id', 'measurement_datetime_hour'])\n","\n","test_df = merge_with_main_table(folder = 'test', main=test_df, sub = device_test_df, keys=['person_id', 'measurement_datetime_hour'])\n","\n","test_df = merge_with_main_table(folder = 'test', main=test_df, sub = obs_test_df, keys=['person_id', 'measurement_datetime_hour'])\n","\n","# test_df[['drug_concept_id_ceftolozane', 'drug_concept_id_isoproterenol', \\\n","#          'drug_concept_id_nitrofurantoin', 'route_concept_id_Rectal']]=0\n","\n","# cols_order = list(train_df.columns)\n","# cols_order.remove('SepsisLabel')\n","# test_df = test_df[cols_order]\n","\n","cols_test = list(test_df.columns)\n","cols_test.remove('person_id')\n","cols_test.remove('measurement_datetime')\n","cols_test.remove('measurement_datetime_hour')\n","\n","test_df[cols_test] = test_df.groupby(['person_id'])[cols_test].ffill()\n","test_df[cols_test] = test_df.groupby(['person_id'])[cols_test].bfill()\n","test_df[['gender', 'procedure', 'device', 'admission_reason']] = \\\n","  test_df[['gender', 'procedure', 'device', 'admission_reason']].fillna('unknown')\n","test_df = test_df.fillna(0) # no data imputation possible\n","\n","test_df = test_df.reset_index()\n","test_df.drop(['index'], axis=1, inplace=True)\n","# test_df['categorical_column_name'] = test_df['categorical_column_name'].fillna('unknown_category')"]},{"cell_type":"code","source":["# some columns are not there in test_df, some are not in train_df, run model on intersection\n","intersection_test = list(set(test_df.columns).intersection(set(train_df.columns)))\n","intersection_train = intersection_test + ['SepsisLabel']\n","\n","# re-order accroding to train dataset (otherwise XGBoost prediction won't work)\n","train_df = train_df[intersection_train]\n","test_df = test_df[intersection_test]"],"metadata":{"id":"ZK0Ux74PraJo","executionInfo":{"status":"ok","timestamp":1764388027483,"user_tz":420,"elapsed":98,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}}},"id":"ZK0Ux74PraJo","execution_count":33,"outputs":[]},{"cell_type":"code","execution_count":35,"id":"2df16486","metadata":{"papermill":{"duration":0.010752,"end_time":"2025-01-31T23:25:00.519733","exception":false,"start_time":"2025-01-31T23:25:00.508981","status":"completed"},"tags":[],"id":"2df16486","executionInfo":{"status":"ok","timestamp":1764388071261,"user_tz":420,"elapsed":32,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}}},"outputs":[],"source":["# define list of features columns\n","cols_feat = list(train_df.columns)\n","\n","cols_feat.remove('person_id')\n","cols_feat.remove('measurement_datetime')\n","cols_feat.remove('SepsisLabel')\n","cols_feat.remove('measurement_datetime_hour')\n","\n","# define list of categorical columns\n","non_numeric_columns = train_df.select_dtypes(exclude=['number']).columns.tolist()\n","non_numeric_columns.remove('measurement_datetime')\n","non_numeric_columns.remove('measurement_datetime_hour')\n","\n","# define list of integer columns\n","integer_columns = train_df.select_dtypes(include=['int', 'int64', 'int32']).columns.tolist()\n","\n","# define float + int column list\n","cols_num = list(set(cols_feat) - set(non_numeric_columns))"]},{"cell_type":"code","source":["# data augmentation strategy\n","if data_augmentation == 'subsample-majority':\n","  train_df_0 = train_df[train_df['SepsisLabel'] == 0]\n","  train_df_0 = train_df_0.sample(frac=0.2, random_state=1) #0.05\n","  train_df_1 = train_df[train_df['SepsisLabel'] == 1]\n","  train_df_pruned = pd.concat([train_df_0, train_df_1], axis=0)\n","\n","  # test_df_pruned = test_df.sort_values(by=['person_id', 'measurement_datetime'])\n","  # test_df_pruned = test_df.copy()\n","\n","\n","  # X_train = np.array(train_df_pruned[cols_feat].values)\n","  # y_train = np.array(train_df_pruned['SepsisLabel'].values)\n","  X_train = train_df_pruned[cols_feat]\n","  y_train = train_df_pruned['SepsisLabel']\n","\n","elif data_augmentation == 'smotenc':\n","  sm = SMOTENC(categorical_features=non_numeric_columns, random_state=42)\n","  X = train_df[cols_feat]\n","  y = train_df['SepsisLabel']\n","  X_train, y_train = sm.fit_resample(X, y)\n","\n","elif data_augmentation == 'adasyn':\n","  adasyn = ADASYN(sampling_strategy='minority', random_state=42)\n","  encoder_ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n","  encoded_nominal = encoder_ohe.fit_transform(train_df[non_numeric_columns])\n","  encoded_nominal_df = pd.DataFrame(encoded_nominal, columns=encoder_ohe.get_feature_names_out(non_numeric_columns))\n","  X = pd.concat([train_df[cols_num], encoded_nominal_df], axis=1)\n","  y = train_df['SepsisLabel']\n","  X_train, y_train  = adasyn.fit_resample(X,y)\n","\n","else:\n","  X_train = train_df[cols_feat]\n","  y_train = train_df['SepsisLabel']\n","\n","if data_augmentation != 'adasyn':\n","  for col in ['gender', 'procedure', 'device', 'admission_reason']:\n","          X_train[col] = X_train[col].astype('category')"],"metadata":{"id":"X8TqqcWz2bC-","executionInfo":{"status":"ok","timestamp":1764388072644,"user_tz":420,"elapsed":369,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ef80e0dc-3977-4b95-f073-62c36afc75c1"},"id":"X8TqqcWz2bC-","execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3892969221.py:38: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  X_train[col] = X_train[col].astype('category')\n","/tmp/ipython-input-3892969221.py:38: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  X_train[col] = X_train[col].astype('category')\n","/tmp/ipython-input-3892969221.py:38: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  X_train[col] = X_train[col].astype('category')\n","/tmp/ipython-input-3892969221.py:38: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  X_train[col] = X_train[col].astype('category')\n"]}]},{"cell_type":"code","execution_count":37,"id":"3b05b90f","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:25:31.897383Z","iopub.status.busy":"2025-01-31T23:25:31.897020Z","iopub.status.idle":"2025-01-31T23:25:31.902595Z","shell.execute_reply":"2025-01-31T23:25:31.901300Z"},"papermill":{"duration":0.020928,"end_time":"2025-01-31T23:25:31.904481","exception":false,"start_time":"2025-01-31T23:25:31.883553","status":"completed"},"tags":[],"id":"3b05b90f","executionInfo":{"status":"ok","timestamp":1764388081157,"user_tz":420,"elapsed":2,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}}},"outputs":[],"source":["# Define the objective function for Bayesian Optimization\n","def catboost_cv(depth, learning_rate, iterations, subsample, l2_leaf_reg):\n","    # Convert hyperparameters to the right format\n","    depth = int(depth)\n","    iterations = int(iterations)\n","    l2_leaf_reg = int(l2_leaf_reg)\n","    # Initialize the CatBoost model (CatBoostRegressor for regression)\n","    if data_augmentation != 'adasyn':\n","      model = CatBoostClassifier(\n","          depth=depth,\n","          learning_rate=learning_rate,\n","          iterations=iterations,\n","          subsample=subsample,\n","          l2_leaf_reg=l2_leaf_reg,\n","          verbose=False,\n","          cat_features=non_numeric_columns,\n","          max_ctr_complexity=2,\n","          border_count = 64,\n","          task_type='GPU',\n","          early_stopping_rounds=50,\n","          bootstrap_type = 'Bernoulli'\n","        )\n","    else:\n","      model = CatBoostClassifier(\n","          depth=depth,\n","          learning_rate=learning_rate,\n","          iterations=iterations,\n","          subsample=subsample,\n","          l2_leaf_reg=l2_leaf_reg,\n","          verbose=False,\n","          max_ctr_complexity=2,\n","          border_count = 64,\n","          task_type='GPU',\n","          early_stopping_rounds=50,\n","          bootstrap_type = 'Bernoulli')\n","\n","\n","    # Perform cross-validation and return the mean R-squared score (for regression)\n","    cross_val_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n","\n","    return cross_val_scores.mean()"]},{"cell_type":"code","execution_count":38,"id":"5455a455","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:25:31.932018Z","iopub.status.busy":"2025-01-31T23:25:31.931599Z","iopub.status.idle":"2025-01-31T23:39:19.986796Z","shell.execute_reply":"2025-01-31T23:39:19.985386Z"},"papermill":{"duration":828.084926,"end_time":"2025-01-31T23:39:20.001543","exception":false,"start_time":"2025-01-31T23:25:31.916617","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"5455a455","outputId":"18e9fc22-34db-4fe1-f625-d43521880486","executionInfo":{"status":"ok","timestamp":1764389121220,"user_tz":420,"elapsed":1039434,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["|   iter    |  target   |   depth   | learni... | iterat... | subsample | l2_lea... |\n","-------------------------------------------------------------------------------------\n","| \u001b[39m1        \u001b[39m | \u001b[39m0.8194378\u001b[39m | \u001b[39m6.5021320\u001b[39m | \u001b[39m0.1449039\u001b[39m | \u001b[39m500.03431\u001b[39m | \u001b[39m0.2209330\u001b[39m | \u001b[39m2.1740471\u001b[39m |\n","| \u001b[35m2        \u001b[39m | \u001b[35m0.8957025\u001b[39m | \u001b[35m4.5540315\u001b[39m | \u001b[35m0.0396932\u001b[39m | \u001b[35m603.66821\u001b[39m | \u001b[35m0.2587069\u001b[39m | \u001b[35m5.3105338\u001b[39m |\n","| \u001b[39m3        \u001b[39m | \u001b[39m0.8308238\u001b[39m | \u001b[39m6.5151670\u001b[39m | \u001b[39m0.1379882\u001b[39m | \u001b[39m561.33567\u001b[39m | \u001b[39m0.4512469\u001b[39m | \u001b[39m1.2191007\u001b[39m |\n","| \u001b[39m4        \u001b[39m | \u001b[39m0.8464114\u001b[39m | \u001b[39m8.0228050\u001b[39m | \u001b[39m0.0852090\u001b[39m | \u001b[39m667.60694\u001b[39m | \u001b[39m0.1561547\u001b[39m | \u001b[39m2.5848119\u001b[39m |\n","| \u001b[39m5        \u001b[39m | \u001b[39m0.8674865\u001b[39m | \u001b[39m8.8044674\u001b[39m | \u001b[39m0.1937475\u001b[39m | \u001b[39m594.02725\u001b[39m | \u001b[39m0.3769290\u001b[39m | \u001b[39m8.0111132\u001b[39m |\n","| \u001b[39m6        \u001b[39m | \u001b[39m0.8649584\u001b[39m | \u001b[39m4.5126962\u001b[39m | \u001b[39m0.0695672\u001b[39m | \u001b[39m602.68445\u001b[39m | \u001b[39m0.3538248\u001b[39m | \u001b[39m5.3036122\u001b[39m |\n","| \u001b[35m7        \u001b[39m | \u001b[35m0.9125495\u001b[39m | \u001b[35m5.1191626\u001b[39m | \u001b[35m0.0160326\u001b[39m | \u001b[35m604.12574\u001b[39m | \u001b[35m0.1600145\u001b[39m | \u001b[35m5.0343160\u001b[39m |\n","| \u001b[35m8        \u001b[39m | \u001b[35m0.9206020\u001b[39m | \u001b[35m4.7563701\u001b[39m | \u001b[35m0.003    \u001b[39m | \u001b[35m605.73674\u001b[39m | \u001b[35m0.1      \u001b[39m | \u001b[35m5.2662219\u001b[39m |\n","| \u001b[39m9        \u001b[39m | \u001b[39m0.9204665\u001b[39m | \u001b[39m4.9963630\u001b[39m | \u001b[39m0.003    \u001b[39m | \u001b[39m605.80543\u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m2.5556429\u001b[39m |\n","| \u001b[39m10       \u001b[39m | \u001b[39m0.9191432\u001b[39m | \u001b[39m8.1199864\u001b[39m | \u001b[39m0.003    \u001b[39m | \u001b[39m606.50792\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m3.9522829\u001b[39m |\n","| \u001b[39m11       \u001b[39m | \u001b[39m0.8023108\u001b[39m | \u001b[39m6.2231802\u001b[39m | \u001b[39m0.2      \u001b[39m | \u001b[39m610.13409\u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m2.4536900\u001b[39m |\n","| \u001b[39m12       \u001b[39m | \u001b[39m0.9192082\u001b[39m | \u001b[39m8.1574584\u001b[39m | \u001b[39m0.003    \u001b[39m | \u001b[39m604.12172\u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m1.8935336\u001b[39m |\n","| \u001b[35m13       \u001b[39m | \u001b[35m0.9270460\u001b[39m | \u001b[35m10.0     \u001b[39m | \u001b[35m0.003    \u001b[39m | \u001b[35m604.31775\u001b[39m | \u001b[35m0.1      \u001b[39m | \u001b[35m5.7909752\u001b[39m |\n","| \u001b[39m14       \u001b[39m | \u001b[39m0.9236890\u001b[39m | \u001b[39m8.3772270\u001b[39m | \u001b[39m0.003    \u001b[39m | \u001b[39m606.13819\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m8.9177806\u001b[39m |\n","| \u001b[39m15       \u001b[39m | \u001b[39m0.8567798\u001b[39m | \u001b[39m5.9537631\u001b[39m | \u001b[39m0.1209708\u001b[39m | \u001b[39m778.89880\u001b[39m | \u001b[39m0.4396872\u001b[39m | \u001b[39m4.2520217\u001b[39m |\n","=====================================================================================\n"]}],"source":["# Train Catboost using CV Bayesian search\n","# Define the hyperparameter search space with data types\n","param_space_cb = {\n","    'depth': (4, 10),             # Integer values for depth\n","    'learning_rate': (0.003, 0.2),  # Float values for learning rate\n","    'iterations': (500, 800),    # Integer values for iterations\n","    'subsample': (0.1, 0.5),       # Float values for subsample\n","    'l2_leaf_reg': (1, 9)        # Integer values for l2_leaf_reg\n","}\n","\n","# Create the BayesianOptimization object and maximize it\n","bayesian_opt_cb = BayesianOptimization(\n","    f=catboost_cv, pbounds=param_space_cb, random_state=1)\n","bayesian_opt_cb.maximize(init_points=5, n_iter=10)\n","results_cb = pd.DataFrame(bayesian_opt_cb.res)\n","results_cb.sort_values(by='target', ascending=False, inplace=True)"]},{"cell_type":"code","execution_count":39,"id":"0f666227","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:20.034126Z","iopub.status.busy":"2025-01-31T23:39:20.033629Z","iopub.status.idle":"2025-01-31T23:39:20.040618Z","shell.execute_reply":"2025-01-31T23:39:20.039109Z"},"papermill":{"duration":0.025395,"end_time":"2025-01-31T23:39:20.042511","exception":false,"start_time":"2025-01-31T23:39:20.017116","status":"completed"},"tags":[],"id":"0f666227","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764389121226,"user_tz":420,"elapsed":4,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}},"outputId":"4e913378-42a8-45b8-8d4e-31581b072d5c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'depth': 10,\n"," 'learning_rate': np.float64(0.003),\n"," 'iterations': 604,\n"," 'subsample': np.float64(0.1),\n"," 'l2_leaf_reg': 5}"]},"metadata":{},"execution_count":39}],"source":["# Print the best hyperparameters and their corresponding R2 score\n","best_hyperparameters_cb = bayesian_opt_cb.max\n","best_hyperparameters_cb['params'] = {param: int(value) if param in [\n","    'depth', 'iterations', 'l2_leaf_reg'] else value for param, value in best_hyperparameters_cb['params'].items()}\n","# print(&quot;Best hyperparameters:&quot;, best_hyperparameters['params'])\n","# print(f&quot;Best R-squared Score: {best_hyperparameters['target']:.4f}&quot;)\n","best_hyperparameters_cb['params']"]},{"cell_type":"code","execution_count":40,"id":"f0a28ad5","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:20.108457Z","iopub.status.busy":"2025-01-31T23:39:20.108040Z","iopub.status.idle":"2025-01-31T23:39:26.316637Z","shell.execute_reply":"2025-01-31T23:39:26.315450Z"},"papermill":{"duration":6.22504,"end_time":"2025-01-31T23:39:26.318583","exception":false,"start_time":"2025-01-31T23:39:20.093543","status":"completed"},"tags":[],"id":"f0a28ad5","executionInfo":{"status":"ok","timestamp":1764389163316,"user_tz":420,"elapsed":42089,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}}},"outputs":[],"source":["# run CatBoost trained model w/ best hyperparameters on test dataset\n","path_base = '/content/drive/My Drive/ISYE6740_project/results/'\n","fname = path_base + 'test_predictions_alpha=' + str(alpha) + '_CatBoost_augmentation=' \\\n","            + data_augmentation + '_common-features_' + '.csv'\n","#\n","p = best_hyperparameters_cb['params']\n","if data_augmentation != 'adasyn':\n","  clf_best = CatBoostClassifier(verbose = False, depth = p['depth'], iterations=p['iterations'], \\\n","                              l2_leaf_reg = p['l2_leaf_reg'], \\\n","                              learning_rate = p['learning_rate'], subsample = p['subsample'], \\\n","                              cat_features=non_numeric_columns, max_ctr_complexity=2,\n","                              border_count = 64,\n","                              task_type='GPU',\n","                              early_stopping_rounds=50,\n","                              bootstrap_type = 'Bernoulli')\n","else:\n","  clf_best = CatBoostClassifier(verbose = False, depth = p['depth'], iterations=p['iterations'], \\\n","                              l2_leaf_reg = p['l2_leaf_reg'], \\\n","                              learning_rate = p['learning_rate'], subsample = p['subsample'], \\\n","                              max_ctr_complexity=2,\n","                              border_count = 64,\n","                              task_type='GPU',\n","                              early_stopping_rounds=50,\n","                              bootstrap_type = 'Bernoulli')\n","\n","\n","clf_best.fit(X_train, y_train)\n","\n","if data_augmentation != 'adasyn':\n","  X_test = test_df[cols_feat]\n","else:\n","  encoder_ohe_test = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n","  encoded_nominal_test = encoder_ohe_test.fit_transform(test_df[non_numeric_columns])\n","  encoded_nominal_test_df = pd.DataFrame(encoded_nominal_test, columns=encoder_ohe_test.get_feature_names_out(non_numeric_columns))\n","  X_test = pd.concat([test_df[cols_num], encoded_nominal_test_df], axis=1)\n","\n","\n","pred_labels = clf_best.predict_proba(X_test)\n","pred_labels = pred_labels[:,1]\n","\n","# create pandas dataframe for submission\n","# submission = test_df.copy()\n","# s = pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/SepsisLabel_test.csv')\n","submission = test_df.copy()\n","submission['person_id'] = submission['person_id'].astype(str)\n","submission['measurement_datetime'] = submission['measurement_datetime'].astype(str)\n","submission['person_id_datetime'] = submission['person_id'].str.cat(submission['measurement_datetime'], sep='_')\n","submission = submission[['person_id_datetime']]\n","submission['SepsisLabel'] = pred_labels\n","submission.to_csv(fname, index=False)"]},{"cell_type":"code","source":["# XG Boost model\n","\n","# data preparation\n","X_train_xgb = X_train.copy()\n","# fix feature name issue\n","import re\n","regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n","\n","X_train_xgb.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) \\\n","                       else col for col in X_train_xgb.columns.values]\n","\n","# for col in ['gender', 'procedure', 'device', 'admission_reason']:\n","#         X_train_xgb[col] = X_train_xgb[col].astype('category')\n","\n","# X_train_xgb=X_train_xgb[['Base excess in Venous blood by calculation', \\\n","#                          'Base excess in Arterial blood by calculation', 'gender']]\n","\n","\n","def xgb_cv(max_depth, learning_rate, n_estimators, subsample, colsample_bytree, scale_pos_weight, gamma, reg_lambda, alpha):\n","    max_depth = int(max_depth)\n","    n_estimators = int(n_estimators)\n","\n","    model = XGBClassifier(\n","        max_depth=max_depth,\n","        learning_rate=learning_rate,\n","        n_estimators=n_estimators,\n","        subsample = subsample,\n","        colsample_bytree = colsample_bytree,\n","        scale_pos_weight=scale_pos_weight,\n","        min_split_loss = gamma,\n","        reg_lambda = reg_lambda,\n","        alpha = alpha,\n","        enable_categorical=True,\n","        # tree_method='gpu_hist',\n","        # predictor='gpu_predictor',\n","        device='cuda')\n","\n","    cross_val_scores = cross_val_score(model, X_train_xgb, y_train, cv=5, scoring='roc_auc', error_score='raise')\n","\n","    return cross_val_scores.mean()"],"metadata":{"id":"DFwfxkI1D-Di","executionInfo":{"status":"ok","timestamp":1764389163336,"user_tz":420,"elapsed":8,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}}},"id":"DFwfxkI1D-Di","execution_count":41,"outputs":[]},{"cell_type":"code","source":["# Train XGB using CV Bayesain\n","# Define the hyperparameter search space with data types\n","param_space_xgb = {\n","    'max_depth': (3, 10),             # Integer values for depth\n","    'learning_rate': (0.005, 0.2),  # Float values for learning rate\n","    'n_estimators': (100, 1000),    # Integer values for iterations\n","    'subsample': (0.5, 1),       # Float values for subsample\n","    'colsample_bytree': (0.5, 1),\n","    'scale_pos_weight': (0.5, 1),\n","    'gamma': (0, 1),\n","    'reg_lambda': (0, 1),\n","    'alpha': (0, 1)\n","}\n","\n","# Create the BayesianOptimization object and maximize it\n","bayesian_opt_xgb = BayesianOptimization(\n","    f=xgb_cv, pbounds=param_space_xgb, random_state=1)\n","bayesian_opt_xgb.maximize(init_points=5, n_iter=10)\n","results_xgb = pd.DataFrame(bayesian_opt_xgb.res)\n","results_xgb.sort_values(by='target', ascending=False, inplace=True)"],"metadata":{"id":"DcfK3I2jGTaL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764389509791,"user_tz":420,"elapsed":346449,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}},"outputId":"83f13561-73f3-4991-ef7a-4e07e47b09f2"},"id":"DcfK3I2jGTaL","execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["|   iter    |  target   | max_depth | learni... | n_esti... | subsample | colsam... | scale_... |   gamma   | reg_la... |   alpha   |\n","-------------------------------------------------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/xgboost/core.py:774: UserWarning: [04:06:04] WARNING: /workspace/src/common/error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n","Potential solutions:\n","- Use a data structure that matches the device ordinal in the booster.\n","- Set the device for booster before call to inplace_predict.\n","\n","This warning will only be shown once.\n","\n","  return func(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["| \u001b[39m1        \u001b[39m | \u001b[39m0.8734523\u001b[39m | \u001b[39m5.9191540\u001b[39m | \u001b[39m0.1454632\u001b[39m | \u001b[39m100.10293\u001b[39m | \u001b[39m0.6511662\u001b[39m | \u001b[39m0.5733779\u001b[39m | \u001b[39m0.5461692\u001b[39m | \u001b[39m0.1862602\u001b[39m | \u001b[39m0.3455607\u001b[39m | \u001b[39m0.3967674\u001b[39m |\n","| \u001b[35m2        \u001b[39m | \u001b[35m0.8844331\u001b[39m | \u001b[35m6.7717171\u001b[39m | \u001b[35m0.0867429\u001b[39m | \u001b[35m716.69755\u001b[39m | \u001b[35m0.6022261\u001b[39m | \u001b[35m0.9390587\u001b[39m | \u001b[35m0.5136937\u001b[39m | \u001b[35m0.6704675\u001b[39m | \u001b[35m0.4173048\u001b[39m | \u001b[35m0.5586898\u001b[39m |\n","| \u001b[39m3        \u001b[39m | \u001b[39m0.8364379\u001b[39m | \u001b[39m3.9827085\u001b[39m | \u001b[39m0.0436297\u001b[39m | \u001b[39m820.67011\u001b[39m | \u001b[39m0.9841307\u001b[39m | \u001b[39m0.6567120\u001b[39m | \u001b[39m0.8461613\u001b[39m | \u001b[39m0.8763891\u001b[39m | \u001b[39m0.8946066\u001b[39m | \u001b[39m0.0850442\u001b[39m |\n","| \u001b[39m4        \u001b[39m | \u001b[39m0.8554048\u001b[39m | \u001b[39m3.2733834\u001b[39m | \u001b[39m0.0381169\u001b[39m | \u001b[39m890.32825\u001b[39m | \u001b[39m0.5491734\u001b[39m | \u001b[39m0.7105538\u001b[39m | \u001b[39m0.9789447\u001b[39m | \u001b[39m0.5331652\u001b[39m | \u001b[39m0.6918771\u001b[39m | \u001b[39m0.3155156\u001b[39m |\n","| \u001b[39m5        \u001b[39m | \u001b[39m0.8789997\u001b[39m | \u001b[39m7.8055064\u001b[39m | \u001b[39m0.1677520\u001b[39m | \u001b[39m116.45944\u001b[39m | \u001b[39m0.8750721\u001b[39m | \u001b[39m0.9944305\u001b[39m | \u001b[39m0.8740828\u001b[39m | \u001b[39m0.2804439\u001b[39m | \u001b[39m0.7892793\u001b[39m | \u001b[39m0.1032260\u001b[39m |\n","| \u001b[35m6        \u001b[39m | \u001b[35m0.8982299\u001b[39m | \u001b[35m6.6050969\u001b[39m | \u001b[35m0.0090575\u001b[39m | \u001b[35m717.73971\u001b[39m | \u001b[35m0.6621636\u001b[39m | \u001b[35m0.7616417\u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m0.7084952\u001b[39m | \u001b[35m0.0319017\u001b[39m | \u001b[35m0.2861136\u001b[39m |\n","| \u001b[39m7        \u001b[39m | \u001b[39m0.8528017\u001b[39m | \u001b[39m6.1750606\u001b[39m | \u001b[39m0.1023629\u001b[39m | \u001b[39m720.05255\u001b[39m | \u001b[39m0.8756582\u001b[39m | \u001b[39m0.5761292\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.7173602\u001b[39m | \u001b[39m0.0075771\u001b[39m | \u001b[39m0.0275662\u001b[39m |\n","| \u001b[35m8        \u001b[39m | \u001b[35m0.9103964\u001b[39m | \u001b[35m7.5554213\u001b[39m | \u001b[35m0.005    \u001b[39m | \u001b[35m717.51494\u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m0.0      \u001b[39m | \u001b[35m0.0      \u001b[39m |\n","| \u001b[39m9        \u001b[39m | \u001b[39m0.9080985\u001b[39m | \u001b[39m8.3199025\u001b[39m | \u001b[39m0.005    \u001b[39m | \u001b[39m717.64995\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m0.0      \u001b[39m |\n","| \u001b[35m10       \u001b[39m | \u001b[35m0.9179296\u001b[39m | \u001b[35m9.4340521\u001b[39m | \u001b[35m0.005    \u001b[39m | \u001b[35m717.76502\u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m0.5      \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m0.0      \u001b[39m | \u001b[35m1.0      \u001b[39m |\n","| \u001b[39m11       \u001b[39m | \u001b[39m0.9166381\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.005    \u001b[39m | \u001b[39m716.02207\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m0.0      \u001b[39m |\n","| \u001b[39m12       \u001b[39m | \u001b[39m0.9177130\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.005    \u001b[39m | \u001b[39m713.07981\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m1.0      \u001b[39m |\n","| \u001b[39m13       \u001b[39m | \u001b[39m0.9174532\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.005    \u001b[39m | \u001b[39m710.44753\u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m0.0      \u001b[39m |\n","| \u001b[39m14       \u001b[39m | \u001b[39m0.9068287\u001b[39m | \u001b[39m7.3265237\u001b[39m | \u001b[39m0.005    \u001b[39m | \u001b[39m710.35515\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m1.0      \u001b[39m |\n","| \u001b[39m15       \u001b[39m | \u001b[39m0.9040174\u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.2      \u001b[39m | \u001b[39m707.65109\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m1.0      \u001b[39m |\n","=====================================================================================================================================\n"]}]},{"cell_type":"code","source":["# Print the best hyperparameters and their corresponding R2 score\n","best_hyperparameters_xgb = bayesian_opt_xgb.max\n","best_hyperparameters_xgb['params'] = {param: int(value) if param in [\n","    'depth', 'iterations', 'l2_leaf_reg'] else value for param, value in best_hyperparameters_xgb['params'].items()}\n","# print(&quot;Best hyperparameters:&quot;, best_hyperparameters['params'])\n","# print(f&quot;Best R-squared Score: {best_hyperparameters['target']:.4f}&quot;)\n","best_hyperparameters_xgb['params']"],"metadata":{"id":"dwB-gVijHJa1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764389509813,"user_tz":420,"elapsed":20,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}},"outputId":"1a22d516-2246-44c2-a1db-1fc804f01385"},"id":"dwB-gVijHJa1","execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'max_depth': np.float64(9.434052139199991),\n"," 'learning_rate': np.float64(0.005),\n"," 'n_estimators': np.float64(717.7650234964225),\n"," 'subsample': np.float64(0.5),\n"," 'colsample_bytree': np.float64(0.5),\n"," 'scale_pos_weight': np.float64(1.0),\n"," 'gamma': np.float64(1.0),\n"," 'reg_lambda': np.float64(0.0),\n"," 'alpha': np.float64(1.0)}"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["# run XGBoost trained model w/ best hyperparameters on test dataset\n","path_base = '/content/drive/My Drive/ISYE6740_project/results/'\n","fname = path_base + 'test_predictions_alpha=' + str(alpha) + '_XGBoost_augmentation=' \\\n","            + data_augmentation + '_common-features_' +  '.csv'\n","#\n","p = best_hyperparameters_xgb['params']\n","xgb_best = XGBClassifier(verbose = False,\n","                         max_depth = int(p['max_depth']),\n","                         learning_rate = p['learning_rate'],\n","                         n_estimators = int(p['n_estimators']),\n","                         subsample = p['subsample'],\n","                         colsample_bytree = p['colsample_bytree'],\n","                         scale_pos_weight = p['scale_pos_weight'],\n","                         min_split_loss = p['gamma'],\n","                         reg_lambda = p['reg_lambda'],\n","                         alpha = p['alpha'],\n","                         enable_categorical=True,\n","                        #  tree_method='gpu_hist',\n","                        # predictor='gpu_predictor',\n","                        device='cuda')\n","\n","\n","xgb_best.fit(X_train_xgb, y_train)\n","\n","X_test_xgb = X_test.copy()\n","\n","X_test_xgb.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) \\\n","                       else col for col in X_test_xgb.columns.values]\n","\n","if data_augmentation != 'adasyn':\n","  for col in ['gender', 'procedure', 'device', 'admission_reason']:\n","          X_test_xgb[col] = X_test_xgb[col].astype('category')\n","\n","pred_labels = xgb_best.predict_proba(X_test_xgb)\n","pred_labels = pred_labels[:,1]\n","\n","# create pandas dataframe for submission\n","# submission = test_df.copy()\n","# s = pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/SepsisLabel_test.csv')\n","submission = test_df.copy()\n","submission['person_id'] = submission['person_id'].astype(str)\n","submission['measurement_datetime'] = submission['measurement_datetime'].astype(str)\n","submission['person_id_datetime'] = submission['person_id'].str.cat(submission['measurement_datetime'], sep='_')\n","submission = submission[['person_id_datetime']]\n","submission['SepsisLabel'] = pred_labels\n","submission.to_csv(fname, index=False)"],"metadata":{"id":"312Ck0mPyaJa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764389518629,"user_tz":420,"elapsed":8815,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}},"outputId":"5f98f49d-882c-4203-9997-9bc3a246a1d0"},"id":"312Ck0mPyaJa","execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [04:11:50] WARNING: /workspace/src/learner.cc:790: \n","Parameters: { \"verbose\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]}]},{"cell_type":"code","execution_count":45,"id":"5c38fd00","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:25:00.603077Z","iopub.status.busy":"2025-01-31T23:25:00.602650Z","iopub.status.idle":"2025-01-31T23:25:03.549432Z","shell.execute_reply":"2025-01-31T23:25:03.548236Z"},"papermill":{"duration":2.960887,"end_time":"2025-01-31T23:25:03.551613","exception":false,"start_time":"2025-01-31T23:25:00.590726","status":"completed"},"tags":[],"id":"5c38fd00","executionInfo":{"status":"ok","timestamp":1764389518632,"user_tz":420,"elapsed":1,"user":{"displayName":"Nicola Ciocchini","userId":"10639498660130911526"}}},"outputs":[],"source":["# # pre-process data for NN\n","# batch_size = 512 # The number of samples per batch\n","\n","# # random subset to remove label bias (1 are just 2% of total label, difficult to train)\n","\n","# train_df_0 = train_df[train_df['SepsisLabel'] == 0]\n","# # train_df_0 = train_df_0.sample(frac=0.2, random_state=1) #0.05\n","# train_df_1 = train_df[train_df['SepsisLabel'] == 1]\n","\n","# train_df_pruned = pd.concat([train_df_0, train_df_1], axis=0)\n","# # train_df_pruned = train_df_pruned.sort_values(by=['person_id', 'measurement_datetime'])\n","\n","# class MyDataset(torch.utils.data.Dataset):\n","#     def __init__(self, X, y):\n","#         self.X = X\n","#         self.y = y\n","\n","#     def __len__(self):\n","#         return len(self.X)\n","\n","#     def __getitem__(self, idx):\n","#         return self.X[idx], self.y[idx]\n","\n","# # prepare data for ML\n","# cols_feat = list(train_df.columns)\n","\n","# cols_feat.remove('person_id')\n","# cols_feat.remove('measurement_datetime')\n","# cols_feat.remove('SepsisLabel')\n","# cols_feat.remove('measurement_datetime_hour')\n","\n","# # manually removed features\n","# col_rem = ['Base excess in Venous blood by calculation', 'Base excess in Arterial blood by calculation', \\\n","#           'Phosphate [Moles/volume] in Serum or Plasma', 'Bilirubin.total [Moles/volume] in Serum or Plasma',\\\n","#           'Potassium [Moles/volume] in Blood', \\\n","#           'Neutrophil Ab [Units/volume] in Serum', 'Bicarbonate [Moles/volume] in Arterial blood', \\\n","#           'Systolic blood pressure', 'Diastolic blood pressure', 'Respiratory rate', 'Heart rate', \\\n","#           'Measurement of oxygen saturation at periphery', 'Oxygen/Gas total [Pure volume fraction] Inhaled gas', \\\n","#           'Capillary refill [Time]']\n","\n","# for col in col_rem:\n","#     cols_feat.remove(col)\n","\n","# # cols_feat.remove('visit_occurrence_id')\n","\n","# cols_all = cols_feat.copy()\n","# cols_all.append('SepsisLabel')\n","\n","# # normalize\n","# scaler = MinMaxScaler()\n","# train_df_pruned[cols_feat] = scaler.fit_transform(train_df_pruned[cols_feat])\n","# # train_df_pruned[cols_feat] = (train_df_pruned[cols_feat] - train_df_pruned[cols_feat].mean()) / train_df_pruned[cols_feat].std()\n","\n","# # split in validation and train dataset\n","# t_df, v_df = train_test_split(train_df_pruned, test_size=0.4, random_state=11)\n","\n","# t_c = MyDataset(torch.tensor(t_df[cols_feat].values, dtype=torch.float32), torch.tensor(t_df['SepsisLabel'].values, dtype=torch.long))\n","# v_c = MyDataset(torch.tensor(v_df[cols_feat].values, dtype=torch.float32), torch.tensor(v_df['SepsisLabel'].values, dtype=torch.long))\n","\n","# # t_pt = torch.tensor(t_df[cols_all].values)\n","# # v_pt = torch.tensor(v_df[cols_all].values)\n","\n","# # Create the data loaders for batching and shuffling the data\n","# train_loader = torch.utils.data.DataLoader(t_c, batch_size=batch_size, shuffle=True) # The training loader\n","# test_loader = torch.utils.data.DataLoader(v_c, batch_size=len(v_df), shuffle=False) # The test loader\n"]},{"cell_type":"code","execution_count":null,"id":"6d4d5244","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:25:03.636433Z","iopub.status.busy":"2025-01-31T23:25:03.636052Z","iopub.status.idle":"2025-01-31T23:25:03.640542Z","shell.execute_reply":"2025-01-31T23:25:03.639319Z"},"papermill":{"duration":0.019266,"end_time":"2025-01-31T23:25:03.642682","exception":false,"start_time":"2025-01-31T23:25:03.623416","status":"completed"},"tags":[],"id":"6d4d5244"},"outputs":[],"source":["# #resampling strategy\n","# smote=SMOTE(sampling_strategy='minority')\n","# x_smote,y_smote=smote.fit_resample(t_df[cols_feat],t_df['SepsisLabel'])\n","# # y_t.value_counts()\n","\n","# adasyn = ADASYN(sampling_strategy='minority')\n","# x_adasyn, y_adasyn = adasyn.fit_resample(t_df[cols_feat],t_df['SepsisLabel'])\n","\n","# blsmote = BorderlineSMOTE(sampling_strategy='minority', kind='borderline-1')\n","# x_blsmote, y_blsmote = blsmote.fit_resample(t_df[cols_feat],t_df['SepsisLabel'])\n","\n","# # smote_enn = SMOTEENN()  # commented----too slow\n","# # x_smote_enn, y_smote_enn = smote_enn.fit_resample(t_df[cols_feat],t_df['SepsisLabel'])\n","\n","# # smt = SMOTETomek(sampling_strategy='auto')  # commented----too slow\n","# # x_smt, y_smt = smt.fit_resample(t_df[cols_feat],t_df['SepsisLabel'])"]},{"cell_type":"code","execution_count":null,"id":"cd4ca34f","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:25:03.666270Z","iopub.status.busy":"2025-01-31T23:25:03.665899Z","iopub.status.idle":"2025-01-31T23:25:03.670107Z","shell.execute_reply":"2025-01-31T23:25:03.669006Z"},"papermill":{"duration":0.018579,"end_time":"2025-01-31T23:25:03.672410","exception":false,"start_time":"2025-01-31T23:25:03.653831","status":"completed"},"tags":[],"id":"cd4ca34f"},"outputs":[],"source":["# # search PCA that explains 99% of data\n","# n_comp_list = [50]\n","# for n_comp in n_comp_list:\n","#     pca = PCA(n_components=n_comp)\n","#     principalComponents = pca.fit_transform(t_df[cols_feat])\n","#     var = np.sum(pca.explained_variance_ratio_)\n","#     print('# comp = ' + str(n_comp) + ', explained variance = ' + str(var))\n","\n","# x_pca_train = pd.DataFrame(data = principalComponents)\n","# x_pca_val = pd.DataFrame(data = pca.transform(v_df[cols_feat]))"]},{"cell_type":"code","execution_count":null,"id":"0b19765e","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:25:30.632225Z","iopub.status.busy":"2025-01-31T23:25:30.631778Z","iopub.status.idle":"2025-01-31T23:25:31.218190Z","shell.execute_reply":"2025-01-31T23:25:31.216996Z"},"papermill":{"duration":0.601593,"end_time":"2025-01-31T23:25:31.220309","exception":false,"start_time":"2025-01-31T23:25:30.618716","status":"completed"},"tags":[],"id":"0b19765e"},"outputs":[],"source":["# # pre-process data for NN\n","# # batch_size = 128 # The number of samples per batch\n","\n","# # random subset to remove label bias (1 are just 2% of total label, difficult to train)\n","\n","# # train_df_0 = train_df[train_df['SepsisLabel'] == 0]\n","# # # train_df_0 = train_df_0.sample(frac=0.2, random_state=1) #0.05\n","# # train_df_1 = train_df[train_df['SepsisLabel'] == 1]\n","\n","# # train_df_pruned = pd.concat([train_df_0, train_df_1], axis=0)\n","# # test_df_pruned = test_df.sort_values(by=['person_id', 'measurement_datetime'])\n","# test_df_pruned = test_df.copy()\n","\n","# class MyDataset(torch.utils.data.Dataset):\n","#     def __init__(self, X, y):\n","#         self.X = X\n","#         self.y = y\n","\n","#     def __len__(self):\n","#         return len(self.X)\n","\n","#     def __getitem__(self, idx):\n","#         return self.X[idx], self.y[idx]\n","\n","# # prepare data for ML\n","# cols_feat = list(test_df.columns)\n","\n","# cols_feat.remove('person_id')\n","# cols_feat.remove('measurement_datetime')\n","# # cols_feat.remove('SepsisLabel')\n","# cols_feat.remove('measurement_datetime_hour')\n","# # cols_feat.remove('visit_occurrence_id')\n","\n","# #manually removed features\n","# col_rem = ['Base excess in Venous blood by calculation', 'Base excess in Arterial blood by calculation', \\\n","#           'Phosphate [Moles/volume] in Serum or Plasma', 'Bilirubin.total [Moles/volume] in Serum or Plasma',\\\n","#           'Potassium [Moles/volume] in Blood', \\\n","#           'Neutrophil Ab [Units/volume] in Serum', 'Bicarbonate [Moles/volume] in Arterial blood', \\\n","#           'Systolic blood pressure', 'Diastolic blood pressure', 'Respiratory rate', 'Heart rate', \\\n","#           'Measurement of oxygen saturation at periphery', 'Oxygen/Gas total [Pure volume fraction] Inhaled gas', \\\n","#           'Capillary refill [Time]']\n","\n","# for col in col_rem:\n","#     cols_feat.remove(col)\n","\n","# # normalize\n","# # scaler = MinMaxScaler()\n","# test_df_pruned[cols_feat] = scaler.transform(test_df_pruned[cols_feat])\n","# # train_df_pruned[cols_feat] = (train_df_pruned[cols_feat] - train_df_pruned[cols_feat].mean()) / train_df_pruned[cols_feat].std()\n","\n","\n","# # tt_c = MyDataset(torch.tensor(test_df_pruned[cols_feat].values, dtype=torch.float32))\n","\n","# # t_pt = torch.tensor(t_df[cols_all].values)\n","# # v_pt = torch.tensor(v_df[cols_all].values)\n","\n","# # Create the data loaders for batching and shuffling the data\n","# test_final_loader = torch.utils.data.DataLoader(torch.tensor(test_df_pruned[cols_feat].values, dtype=torch.float32), batch_size=len(test_df_pruned), shuffle=False) # The test loader\n","\n","# # x_pca_test = pd.DataFrame(data = pca.transform(test_df_pruned[cols_feat]))"]},{"cell_type":"code","execution_count":null,"id":"d0db47b6","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:25:31.278193Z","iopub.status.busy":"2025-01-31T23:25:31.277774Z","iopub.status.idle":"2025-01-31T23:25:31.284506Z","shell.execute_reply":"2025-01-31T23:25:31.283323Z"},"papermill":{"duration":0.020617,"end_time":"2025-01-31T23:25:31.286346","exception":false,"start_time":"2025-01-31T23:25:31.265729","status":"completed"},"tags":[],"id":"d0db47b6"},"outputs":[],"source":["# def xgb_train_and_validate(X_train, y_train, X_val, y_val, max_depth=15, subsample=1, \\\n","#                            colsample_bytree=1, scale_pos_weight = 1, booster='gbtree', \\\n","#                           gamma = 0, eta = 0.3, lam = 1, alpha = 0):\n","\n","#     xgb = XGBClassifier(max_depth=max_depth, subsample = subsample, \\\n","#                         colsample_bytree = colsample_bytree, scale_pos_weight=scale_pos_weight, \\\n","#                        booster = booster, min_split_loss = gamma, eta = eta, reg_lambda = lam, alpha = alpha)\n","#     xgb.fit(X_train, y_train)\n","#     y_xgb_pred = xgb.predict(X_val)\n","\n","#     cm = confusion_matrix(y_val, y_xgb_pred)\n","#     tn, fp, fn, tp = cm.ravel()\n","#     # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","#     # plt.xlabel('Predicted')\n","#     # plt.ylabel('Actual')\n","#     # plt.title('Confusion Matrix')\n","#     # plt.show()\n","\n","#     fp_tp_ratio = fp/(fp+tp)\n","#     print('fp/(fp+tp) = ' + str(fp_tp_ratio))\n","\n","#     return (fp_tp_ratio, xgb)"]},{"cell_type":"code","execution_count":null,"id":"5ae0db05","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:25:31.311104Z","iopub.status.busy":"2025-01-31T23:25:31.310688Z","iopub.status.idle":"2025-01-31T23:25:31.316247Z","shell.execute_reply":"2025-01-31T23:25:31.315023Z"},"papermill":{"duration":0.019793,"end_time":"2025-01-31T23:25:31.318142","exception":false,"start_time":"2025-01-31T23:25:31.298349","status":"completed"},"tags":[],"id":"5ae0db05"},"outputs":[],"source":["# # X_train = np.array(t_df[cols_feat].values)\n","# # y_train = np.array(t_df['SepsisLabel'].values)\n","\n","# # X_train = np.array(train_df_pruned[cols_feat].values)\n","# # y_train = np.array(train_df_pruned['SepsisLabel'].values)\n","\n","# sampling_strategy = 'none'\n","\n","# if sampling_strategy == 'smote':\n","#     X_train = np.array(x_smote.values)\n","#     y_train = np.array(y_smote.values)\n","# elif sampling_strategy == 'adasyn':\n","#     X_train = np.array(x_adasyn.values)\n","#     y_train = np.array(y_adasyn.values)\n","# elif sampling_strategy == 'blsmote':\n","#     X_train = np.array(x_blsmote.values)\n","#     y_train = np.array(y_blsmote.values)\n","# elif sampling_strategy == 'enn':\n","#     X_train = np.array(x_smote_enn.values)\n","#     y_train = np.array(y_smote_enn.values)\n","# elif sampling_strategy == 'smt':\n","#     X_train = np.array(x_smt.values)\n","#     y_train = np.array(y_smt.values)\n","# elif sampling_strategy == 'pca':\n","#     X_train = np.array(x_pca_train.values)\n","#     y_train = np.array(t_df['SepsisLabel'].values)\n","# else :\n","#     X_train = np.array(t_df[cols_feat].values)\n","#     y_train = np.array(t_df['SepsisLabel'].values)\n","\n","\n","# # classes_weights = class_weight.compute_sample_weight(\n","# #     class_weight='balanced',\n","# #     y=y_train\n","# # )\n","# if sampling_strategy == 'pca':\n","#     X_val = np.array(x_pca_val.values)\n","#     X_test = np.array(x_pca_test.values)\n","# else:\n","#     X_val = np.array(v_df[cols_feat].values)\n","#     X_test = np.array(test_df_pruned[cols_feat].values)\n","# y_val = np.array(v_df['SepsisLabel'].values)\n","\n","# # best seeting no PCA\n","# max_depth_list = [10, 11, 12, 13, 14, 15]# [12]\n","# subsample_list = [0.5]\n","# colsample_bytree_list = [0.5]\n","# scale_pos_weight_list = [1]\n","# gamma_list = [1] #[0, 1, 5]\n","# eta_list = [0.1] #[0.1, 0.5, 0.8]\n","# lam_list = [0.5] #[0, 0.5, 1]\n","# alp_list = [0] #[0, 0.5, 1]\n","\n","# # max_depth_list = [16]\n","# # subsample_list = [1]\n","# # colsample_bytree_list = [0.5]\n","# # scale_pos_weight_list = [1]\n","# # gamma_list = [1] #[0, 1, 5]\n","# # eta_list = [0.1] #[0.1, 0.5, 0.8]\n","# # lam_list = [0.5] #[0, 0.5, 1]\n","# # alp_list = [0] #[0, 0.5, 1]\n","\n","\n","# data_to_append = []\n","# i = 0\n","# for max_depth in max_depth_list:\n","#     for subsample in subsample_list:\n","#         for colsample_bytree in colsample_bytree_list:\n","#             for scale_pos_weight in scale_pos_weight_list:\n","#                 for gamma in gamma_list:\n","#                     for eta in eta_list:\n","#                         for lam in lam_list:\n","#                             for alp in alp_list:\n","#                                 fp_tp_ratio, xgb = xgb_train_and_validate(X_train, y_train, X_val, y_val, max_depth, \\\n","#                                             subsample, colsample_bytree, scale_pos_weight, \\\n","#                                             booster='gbtree', gamma=gamma, eta=eta, lam = lam, alpha = alp)\n","#                                 new_row = {'max_depth': max_depth, 'subsample': subsample, 'colsample_bytree': \\\n","#                                           colsample_bytree, 'scale_pos_weight': scale_pos_weight, \\\n","#                                            'gamma': gamma, 'eta': eta, 'lambda': lam, 'alpha': alp, \\\n","#                                            'fp_tp_ratio':fp_tp_ratio}\n","#                                 data_to_append.append(new_row)\n","\n","#                                 # run on test dataset\n","#                                 # X_test = np.array(test_df_pruned[cols_feat].values)\n","#                                 pred_labels = xgb.predict_proba(X_test)\n","#                                 pred_labels = pred_labels[:,1]\n","\n","#                                 # create pandas dataframe for submission\n","#                                 # submission = test_df.copy()\n","#                                 # s = pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/SepsisLabel_test.csv')\n","#                                 submission = test_df_pruned.copy()\n","#                                 submission['person_id'] = submission['person_id'].astype(str)\n","#                                 submission['measurement_datetime'] = submission['measurement_datetime'].astype(str)\n","#                                 submission['person_id_datetime'] = submission['person_id'].str.cat(submission['measurement_datetime'], sep='_')\n","#                                 submission = submission[['person_id_datetime']]\n","#                                 submission['SepsisLabel'] = pred_labels\n","#                                 submission.to_csv('A0p3submission_'+str(i) +'.csv', index=False)\n","\n","#                                 i = i+1\n","\n","# r_df = pd.DataFrame(data_to_append)\n","# r_df.to_csv('r_df_4.csv')"]},{"cell_type":"code","execution_count":null,"id":"aa9d20c0","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:25:31.342578Z","iopub.status.busy":"2025-01-31T23:25:31.342223Z","iopub.status.idle":"2025-01-31T23:25:31.347229Z","shell.execute_reply":"2025-01-31T23:25:31.345503Z"},"papermill":{"duration":0.019475,"end_time":"2025-01-31T23:25:31.349373","exception":false,"start_time":"2025-01-31T23:25:31.329898","status":"completed"},"tags":[],"id":"aa9d20c0"},"outputs":[],"source":["# # r_df = r_df.sort_values(by=['fp_tp_ratio'])\n","# # r_df.to_csv('r_df_3.csv')\n","# r_df.head(10)"]},{"cell_type":"code","execution_count":null,"id":"585d0008","metadata":{"papermill":{"duration":0.012134,"end_time":"2025-01-31T23:25:31.373597","exception":false,"start_time":"2025-01-31T23:25:31.361463","status":"completed"},"tags":[],"id":"585d0008"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"4508427b","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:25:31.397976Z","iopub.status.busy":"2025-01-31T23:25:31.397561Z","iopub.status.idle":"2025-01-31T23:25:31.402775Z","shell.execute_reply":"2025-01-31T23:25:31.401387Z"},"papermill":{"duration":0.019647,"end_time":"2025-01-31T23:25:31.404846","exception":false,"start_time":"2025-01-31T23:25:31.385199","status":"completed"},"tags":[],"id":"4508427b"},"outputs":[],"source":["# # catboost grid search\n","# from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","# from sklearn.metrics import classification_report\n","# from sklearn.metrics import make_scorer, f1_score, auc\n","\n","# X_train = np.array(train_df_pruned[cols_feat].values)\n","# y_train = np.array(train_df_pruned['SepsisLabel'].values)\n","\n","# param_grid = {\n","#     'learning_rate': [0.1],\n","#     'depth':[7,8,9,10,11],\n","#     'l2_leaf_reg': [2, 3, 4],\n","#     'boosting_type': ['Ordered', 'Plain']\n","# }\n","\n","# # clf = CatBoostClassifier(iterations=100, learning_rate=0.1, depth = 10, \\\n","# #                          loss_function='CrossEntropy', eval_metric='AUC') #LogLoss\n","\n","# scorer = make_scorer(auc, average='weighted')\n","\n","# clf = CatBoostClassifier(iterations=30, loss_function='CrossEntropy', eval_metric='AUC') #LogLoss\n","\n","# # clf_grid_results = clf.grid_search(grid, X_train, y_train, cv=5, verbose=20)\n","\n","# grid_search = GridSearchCV(clf, param_grid, cv=5, scoring=scorer, n_jobs=-1)\n","# grid_search.fit(X_train, y_train)\n","# print(\"Grid Search - Best Hyperparameters:\", grid_search.best_params_)\n","\n","# # pred_labels = clf_grid_results.predict_proba(X_test)\n","\n","# # pred_labels = pred_labels[:,1]\n","\n","# # # create pandas dataframe for submission\n","# # # submission = test_df.copy()\n","# # # s = pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/SepsisLabel_test.csv')\n","# # submission = test_df_pruned.copy()\n","# # submission['person_id'] = submission['person_id'].astype(str)\n","# # submission['measurement_datetime'] = submission['measurement_datetime'].astype(str)\n","# # submission['person_id_datetime'] = submission['person_id'].str.cat(submission['measurement_datetime'], sep='_')\n","# # submission = submission[['person_id_datetime']]\n","# # submission['SepsisLabel'] = pred_labels\n","# # submission.to_csv('CBA0p3submission_pct-val='+str(pct_val) +'.csv', index=False)"]},{"cell_type":"code","execution_count":null,"id":"38fb9243","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:25:31.429650Z","iopub.status.busy":"2025-01-31T23:25:31.429151Z","iopub.status.idle":"2025-01-31T23:25:31.434906Z","shell.execute_reply":"2025-01-31T23:25:31.433104Z"},"papermill":{"duration":0.020821,"end_time":"2025-01-31T23:25:31.437078","exception":false,"start_time":"2025-01-31T23:25:31.416257","status":"completed"},"tags":[],"id":"38fb9243"},"outputs":[],"source":["# pred_labels = clf.predict_proba(X_test)\n","\n","# pred_labels = pred_labels[:,1]\n","\n","# # create pandas dataframe for submission\n","# # submission = test_df.copy()\n","# # s = pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/SepsisLabel_test.csv')\n","# submission = test_df_pruned.copy()\n","# submission['person_id'] = submission['person_id'].astype(str)\n","# submission['measurement_datetime'] = submission['measurement_datetime'].astype(str)\n","# submission['person_id_datetime'] = submission['person_id'].str.cat(submission['measurement_datetime'], sep='_')\n","# submission = submission[['person_id_datetime']]\n","# submission['SepsisLabel'] = pred_labels\n","# submission.to_csv('CBA0p3submission_'+str(28) +'.csv', index=False)"]},{"cell_type":"code","execution_count":null,"id":"304d8aa9","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:25:31.865360Z","iopub.status.busy":"2025-01-31T23:25:31.864981Z","iopub.status.idle":"2025-01-31T23:25:31.869176Z","shell.execute_reply":"2025-01-31T23:25:31.868044Z"},"papermill":{"duration":0.019407,"end_time":"2025-01-31T23:25:31.871330","exception":false,"start_time":"2025-01-31T23:25:31.851923","status":"completed"},"tags":[],"id":"304d8aa9"},"outputs":[],"source":["# # train with best hyperparameters\n","# r_df_best = r_df.iloc[[0]]\n","# max_depth = r_df_best['max_depth'].iloc[0]\n","# subsample = r_df_best['subsample'].iloc[0]\n","# colsample_bytree = r_df_best['colsample_bytree'].iloc[0]\n","# scale_pos_weight = r_df_best['scale_pos_weight'].iloc[0]\n","\n","# fp_tp_ratio_best, xgb_best = xgb_train_and_validate(X_train, y_train, X_val, y_val, max_depth, \\\n","#                             subsample, colsample_bytree, scale_pos_weight)"]},{"cell_type":"code","execution_count":null,"id":"7f540de8","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:26.427751Z","iopub.status.busy":"2025-01-31T23:39:26.427388Z","iopub.status.idle":"2025-01-31T23:39:26.431616Z","shell.execute_reply":"2025-01-31T23:39:26.430623Z"},"papermill":{"duration":0.101261,"end_time":"2025-01-31T23:39:26.433236","exception":false,"start_time":"2025-01-31T23:39:26.331975","status":"completed"},"tags":[],"id":"7f540de8"},"outputs":[],"source":["# # run on test dataset\n","# X_test = np.array(test_df_pruned[cols_feat].values)\n","# pred_labels = xgb_best.predict_proba(X_test)\n","# pred_labels = pred_labels[:,1]\n","\n","# # create pandas dataframe for submission\n","# # submission = test_df.copy()\n","# # s = pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/SepsisLabel_test.csv')\n","# submission = test_df_pruned.copy()\n","# submission['person_id'] = submission['person_id'].astype(str)\n","# submission['measurement_datetime'] = submission['measurement_datetime'].astype(str)\n","# submission['person_id_datetime'] = submission['person_id'].str.cat(submission['measurement_datetime'], sep='_')\n","# submission = submission[['person_id_datetime']]\n","# submission['SepsisLabel'] = pred_labels\n","# submission.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"id":"dfe299ab","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:26.461435Z","iopub.status.busy":"2025-01-31T23:39:26.461053Z","iopub.status.idle":"2025-01-31T23:39:26.465400Z","shell.execute_reply":"2025-01-31T23:39:26.464285Z"},"papermill":{"duration":0.020814,"end_time":"2025-01-31T23:39:26.467769","exception":false,"start_time":"2025-01-31T23:39:26.446955","status":"completed"},"tags":[],"id":"dfe299ab"},"outputs":[],"source":["# # Define the hyperparameters\n","# # rev1\n","# num_epochs = 3 # The number of times to iterate over the whole dataset\n","# learning_rate = 2e-3 # The learning rate for the optimizer\n","# n_L1 = 15\n","# n_L2 = 5\n","# n_o = 2\n","# l2_reg = 1e-4\n","# derate = 0.01\n","\n","# # # rev2\n","# # num_epochs = 10 # The number of times to iterate over the whole dataset\n","# # learning_rate = 1e-3 # The learning rate for the optimizer\n","# # n_L1 = 40\n","# # n_L2 = 15\n","# # n_o = 2\n","# # l2_reg = 1e-5\n","# # derate = 0.02\n","\n","# class_weights = torch.tensor([derate, 1.0], dtype=torch.float) # to compesnate for bias in label distribution\n"]},{"cell_type":"code","execution_count":null,"id":"9ccdc7de","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:26.495915Z","iopub.status.busy":"2025-01-31T23:39:26.495533Z","iopub.status.idle":"2025-01-31T23:39:26.500357Z","shell.execute_reply":"2025-01-31T23:39:26.499204Z"},"papermill":{"duration":0.020679,"end_time":"2025-01-31T23:39:26.502202","exception":false,"start_time":"2025-01-31T23:39:26.481523","status":"completed"},"tags":[],"id":"9ccdc7de"},"outputs":[],"source":["# # create pytorch object to train NN\n","# class Net(nn.Module):\n","#     def __init__(self):\n","#         super(Net, self).__init__()\n","#         # The network has two fully connected layers\n","#         self.fc1 = nn.Linear(len(cols_feat), n_L1) # The first layer takes the flattened image as input and outputs 512 features\n","#         self.fc2 = nn.Linear(n_L1, n_L2) # The second layer takes the 512 features as input and outputs 10 classes\n","#         # self.fc3 = nn.Linear(n_L2, n_L3)\n","#         self.output_layer = nn.Linear(n_L2, n_o)\n","#         self.sigmoid = nn.Sigmoid()\n","\n","#     def forward(self, x):\n","#         # The forward pass of the network\n","#         # x = x.view(-1, 28*28) # Flatten the image into a vector\n","#         x = F.relu(self.fc1(x)) # Apply the ReLU activation function to the first layer\n","#         x = F.relu(self.fc2(x)) # Apply the second layer\n","#         # x = F.relu(self.fc3(x)) # Apply the second layer\n","#         x = F.softmax(self.output_layer(x), dim=1)\n","#         # x = self.sigmoid(x)\n","#         return x # Return the output logits"]},{"cell_type":"code","execution_count":null,"id":"6ebd960a","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:26.531450Z","iopub.status.busy":"2025-01-31T23:39:26.531048Z","iopub.status.idle":"2025-01-31T23:39:26.535412Z","shell.execute_reply":"2025-01-31T23:39:26.534249Z"},"papermill":{"duration":0.020983,"end_time":"2025-01-31T23:39:26.537198","exception":false,"start_time":"2025-01-31T23:39:26.516215","status":"completed"},"tags":[],"id":"6ebd960a"},"outputs":[],"source":["# # # Create an instance of the model and move it to the device (CPU or GPU)\n","# # device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # Get the device\n","# model = Net() # Move the model to the device\n","# # print(model) # Print the model summary\n","\n","# # Define the loss function and the optimizer\n","# criterion = nn.CrossEntropyLoss(weight = class_weights) # The cross entropy loss for multi-class classification\n","# # criterion = nn.BCELoss(weight = class_weights)\n","# optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_reg) # The stochastic gradient descent optimizer\n","\n","# # Define a function to calculate the accuracy of the model\n","# def accuracy(outputs, labels, plot_confusion=False):\n","#     # The accuracy is the percentage of correct predictions\n","#     _, preds = torch.max(outputs, 1) # Get the predicted classes from the output logits\n","#     o_np = outputs.detach().numpy()\n","#     # pd.DataFrame(o_np).to_csv('calculated_output.csv')\n","#     p = o_np[:,1]/(o_np[:,0]+o_np[:,1])\n","#     # pd.DataFrame(p).to_csv('calculated_probability.csv')\n","#     if plot_confusion:\n","#         cm = confusion_matrix(labels.numpy(), np.argmax(o_np,1))\n","#         tn, fp, fn, tp = cm.ravel()\n","#         sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","#         plt.xlabel('Predicted')\n","#         plt.ylabel('Actual')\n","#         plt.title('Confusion Matrix')\n","#         plt.show()\n","#     auc = roc_auc_score(labels.numpy(), p)\n","#     return tp/(tp+fp) # torch.sum(preds == labels).item() / len(labels)"]},{"cell_type":"code","execution_count":null,"id":"cd2d483b","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:26.565829Z","iopub.status.busy":"2025-01-31T23:39:26.565418Z","iopub.status.idle":"2025-01-31T23:39:26.570691Z","shell.execute_reply":"2025-01-31T23:39:26.569401Z"},"papermill":{"duration":0.021456,"end_time":"2025-01-31T23:39:26.572609","exception":false,"start_time":"2025-01-31T23:39:26.551153","status":"completed"},"tags":[],"id":"cd2d483b"},"outputs":[],"source":["# # Define the training loop\n","# def train(model, train_loader, criterion, optimizer, epoch, plot_confusion=False):\n","#     # Set the model to training mode\n","#     model.train()\n","#     # Initialize the running loss and accuracy\n","#     running_loss = 0.0\n","#     running_acc = 0.0\n","#     # Loop over the batches of data\n","#     for i, (inputs, labels) in enumerate(train_loader):\n","#         # print(i)\n","#         # print(labels)\n","#         # Move the inputs and labels to the device\n","#         # inputs = inputs\n","#         # labels = labels\n","#         # Zero the parameter gradients\n","#         optimizer.zero_grad()\n","#         # Forward pass\n","#         outputs = model(inputs) # Get the output logits from the model\n","#         # print(outputs)\n","#         loss = criterion(outputs, labels) # Calculate the loss\n","#         # print(loss)\n","#         # Backward pass and optimize\n","#         loss.backward() # Compute the gradients\n","#         optimizer.step() # Update the parameters\n","#         # Print the statistics\n","#         running_loss += loss.item() # Accumulate the loss\n","#         try:\n","#            acc = accuracy(outputs, labels, plot_confusion)\n","#         except:\n","#             acc = 0\n","#         running_acc += acc # Accumulate the accuracy\n","#         if (i+1 ) % 200 == 0: # Print every 200 batches\n","#             print(f'Epoch {epoch}, Batch {i + 1}, Loss: {running_loss / 200:.4f}, AUC: {running_acc / 200:.4f}')\n","#             running_loss = 0.0\n","#             running_acc = 0.0\n","\n","# # Define the test loop\n","# def test(model, test_loader, criterion, plot_confusion=True):\n","#     # Set the model to evaluation mode\n","#     model.eval()\n","#     # Initialize the loss and accuracy\n","#     test_loss = 0.0\n","#     test_acc = 0.0\n","#     # Loop over the batches of data\n","#     with torch.no_grad(): # No need to track the gradients\n","#         for inputs, labels in test_loader:\n","#             # Move the inputs and labels to the device\n","#             # inputs = inputs.to(device)\n","#             # labels = labels.to(device)\n","#             # Forward pass\n","#             outputs = model(inputs) # Get the output logits from the model\n","#             loss = criterion(outputs, labels) # Calculate the loss\n","#             # Print the statistics\n","#             try:\n","#                acc = accuracy(outputs, labels, plot_confusion)\n","#             except:\n","#                 acc = 0\n","#             test_loss += loss.item() # Accumulate the loss\n","#             test_acc += acc # Accumulate the accuracy\n","#     # Print the average loss and accuracy\n","#     print(f'Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_acc / len(test_loader):.4f}')\n","#     # pd.DataFrame(np.argmax(outputs.detach().numpy(), 1)).to_csv('output_val.csv')\n","#     # pd.DataFrame(labels.detach().numpy()).to_csv('label_val.csv')\n","# for epoch in range(1, num_epochs + 1):\n","#     # print(epoch)\n","#     train(model, train_loader, criterion, optimizer, epoch) # Train the model\n","#     test(model, test_loader, criterion, plot_confusion=True) # Test the model"]},{"cell_type":"code","execution_count":null,"id":"78f90e2e","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:26.628943Z","iopub.status.busy":"2025-01-31T23:39:26.628550Z","iopub.status.idle":"2025-01-31T23:39:26.633025Z","shell.execute_reply":"2025-01-31T23:39:26.631807Z"},"papermill":{"duration":0.021075,"end_time":"2025-01-31T23:39:26.634892","exception":false,"start_time":"2025-01-31T23:39:26.613817","status":"completed"},"tags":[],"id":"78f90e2e"},"outputs":[],"source":["# from sklearn.ensemble import AdaBoostClassifier\n","# ada = AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=42)\n","\n","# ada.fit(X_train, y_train)\n","\n","# # Make predictions on the test data\n","# y_xgb_pred = ada.predict(X_val)\n","\n","# cm = confusion_matrix(y_val, y_xgb_pred)\n","# tn, fp, fn, tp = cm.ravel()\n","# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","# plt.xlabel('Predicted')\n","# plt.ylabel('Actual')\n","# plt.title('Confusion Matrix')\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"id":"92f97d82","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:26.665064Z","iopub.status.busy":"2025-01-31T23:39:26.664583Z","iopub.status.idle":"2025-01-31T23:39:26.670021Z","shell.execute_reply":"2025-01-31T23:39:26.668106Z"},"papermill":{"duration":0.022662,"end_time":"2025-01-31T23:39:26.672197","exception":false,"start_time":"2025-01-31T23:39:26.649535","status":"completed"},"tags":[],"id":"92f97d82"},"outputs":[],"source":["# X_test = np.array(test_df_pruned[cols_feat].values)\n","# pred_labels = xgb.predict_proba(X_test)\n","# pred_labels = pred_labels[:,1]\n","\n","# # create pandas dataframe for submission\n","# # submission = test_df.copy()\n","# # s = pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/SepsisLabel_test.csv')\n","# submission = test_df_pruned.copy()\n","# submission['person_id'] = submission['person_id'].astype(str)\n","# submission['measurement_datetime'] = submission['measurement_datetime'].astype(str)\n","# submission['person_id_datetime'] = submission['person_id'].str.cat(submission['measurement_datetime'], sep='_')\n","# submission = submission[['person_id_datetime']]\n","# submission['SepsisLabel'] = pred_labels\n","# submission.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"id":"94b3ded0","metadata":{"papermill":{"duration":0.012622,"end_time":"2025-01-31T23:39:26.698318","exception":false,"start_time":"2025-01-31T23:39:26.685696","status":"completed"},"tags":[],"id":"94b3ded0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d6d00a20","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:26.725739Z","iopub.status.busy":"2025-01-31T23:39:26.725345Z","iopub.status.idle":"2025-01-31T23:39:26.729680Z","shell.execute_reply":"2025-01-31T23:39:26.728532Z"},"papermill":{"duration":0.02041,"end_time":"2025-01-31T23:39:26.731665","exception":false,"start_time":"2025-01-31T23:39:26.711255","status":"completed"},"tags":[],"id":"d6d00a20"},"outputs":[],"source":["# test_df.tail(50)"]},{"cell_type":"code","execution_count":null,"id":"a516eeef","metadata":{"execution":{"iopub.execute_input":"2025-01-28T06:49:39.725032Z","iopub.status.busy":"2025-01-28T06:49:39.724617Z","iopub.status.idle":"2025-01-28T06:49:40.167160Z","shell.execute_reply":"2025-01-28T06:49:40.165715Z","shell.execute_reply.started":"2025-01-28T06:49:39.724999Z"},"papermill":{"duration":0.012871,"end_time":"2025-01-31T23:39:26.758613","exception":false,"start_time":"2025-01-31T23:39:26.745742","status":"completed"},"tags":[],"id":"a516eeef"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"46d47db9","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:26.787987Z","iopub.status.busy":"2025-01-31T23:39:26.787566Z","iopub.status.idle":"2025-01-31T23:39:26.792518Z","shell.execute_reply":"2025-01-31T23:39:26.791052Z"},"papermill":{"duration":0.021809,"end_time":"2025-01-31T23:39:26.794460","exception":false,"start_time":"2025-01-31T23:39:26.772651","status":"completed"},"tags":[],"id":"46d47db9"},"outputs":[],"source":["# # evluate model on test dataset\n","\n","# def test_final(model, test_loader, criterion):\n","#     # Set the model to evaluation mode\n","#     model.eval()\n","\n","#     # Loop over the batches of data\n","#     with torch.no_grad(): # No need to track the gradients\n","#         for inputs in test_loader:\n","#             # Move the inputs and labels to the device\n","#             # inputs = inputs.to(device)\n","#             # labels = labels.to(device)\n","#             # Forward pass\n","#             outputs = model(inputs) # Get the output logits from the model\n","#             # loss = criterion(outputs, labels) # Calculate the loss\n","#             # Print the statistics\n","#             # try:\n","#             #    acc = accuracy(outputs, labels)\n","#             # except:\n","#             #     acc = 0\n","#             # test_loss += loss.item() # Accumulate the loss\n","#             # test_acc += acc # Accumulate the accuracy\n","#     # Print the average loss and accuracy\n","#     # print(f'Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_acc / len(test_loader):.4f}')\n","#     # print(inputs)\n","#     o_np = outputs.detach().numpy()\n","#     p = o_np[:,1]/(o_np[:,0]+o_np[:,1])\n","#     return p\n","\n","# pred_labels = test_final(model, test_final_loader, criterion) # Test the model"]},{"cell_type":"code","execution_count":null,"id":"b63cc173","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:26.822311Z","iopub.status.busy":"2025-01-31T23:39:26.821928Z","iopub.status.idle":"2025-01-31T23:39:26.826837Z","shell.execute_reply":"2025-01-31T23:39:26.825480Z"},"papermill":{"duration":0.0211,"end_time":"2025-01-31T23:39:26.828931","exception":false,"start_time":"2025-01-31T23:39:26.807831","status":"completed"},"tags":[],"id":"b63cc173"},"outputs":[],"source":["# # create pandas dataframe for submission\n","# # submission = test_df.copy()\n","# # s = pd.read_csv('/kaggle/input/phems-hackathon-early-sepsis-prediction/testing_data/SepsisLabel_test.csv')\n","# submission = test_df_pruned.copy()\n","# submission['person_id'] = submission['person_id'].astype(str)\n","# submission['measurement_datetime'] = submission['measurement_datetime'].astype(str)\n","# submission['person_id_datetime'] = submission['person_id'].str.cat(submission['measurement_datetime'], sep='_')\n","# submission = submission[['person_id_datetime']]\n","# submission['SepsisLabel'] = pred_labels\n","# submission.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"id":"8998fa84","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:26.856597Z","iopub.status.busy":"2025-01-31T23:39:26.856222Z","iopub.status.idle":"2025-01-31T23:39:26.860422Z","shell.execute_reply":"2025-01-31T23:39:26.859442Z"},"papermill":{"duration":0.020055,"end_time":"2025-01-31T23:39:26.862147","exception":false,"start_time":"2025-01-31T23:39:26.842092","status":"completed"},"tags":[],"id":"8998fa84"},"outputs":[],"source":["# shift by 6 hours and update table\n","# submission_2 = test_df_pruned.copy()\n","# # submission['person_id'] = submission['person_id'].astype(str)\n","# # submission['measurement_datetime'] = submission['measurement_datetime'].astype(str)\n","# # submission['person_id_datetime'] = submission['person_id'].str.cat(submission['measurement_datetime'], sep='_')\n","# # submission = submission[['person_id_datetime']]\n","# submission_2['measurement_datetime_shifted'] = submission_2['measurement_datetime'] + pd.to_timedelta('6 hours')\n","# submission_2['SepsisLabel'] = pred_labels\n","\n","# submission_2_shifted = submission_2[['person_id','measurement_datetime_shifted', 'SepsisLabel']].copy()\n","# submission_2 = submission_2[['person_id', 'measurement_datetime']]\n","\n","# submission_2 = submission_2.merge(submission_2_shifted, left_on=['person_id','measurement_datetime'], \\\n","#                            right_on=['person_id','measurement_datetime_shifted'], how='left')\n","\n","# # submission_2.head()\n","# submission_2['person_id'] = submission_2['person_id'].astype(str)\n","# submission_2['measurement_datetime'] = submission_2['measurement_datetime'].astype(str)\n","# submission_2['person_id_datetime'] = submission_2['person_id'].str.cat(submission_2['measurement_datetime'], sep='_')\n","# submission_2 = submission_2[['person_id_datetime', 'SepsisLabel']]\n","# submission_2['SepsisLabel'] = submission_2['SepsisLabel'].fillna(0)\n","# submission_2.to_csv('submission.csv', index=False)\n","# submission_2.sort_values(by=['person_id_datetime']).head()\n","\n","# # submission_2.head()"]},{"cell_type":"code","execution_count":null,"id":"c2403f82","metadata":{"execution":{"iopub.execute_input":"2025-01-31T23:39:26.892244Z","iopub.status.busy":"2025-01-31T23:39:26.891833Z","iopub.status.idle":"2025-01-31T23:39:26.896282Z","shell.execute_reply":"2025-01-31T23:39:26.895195Z"},"papermill":{"duration":0.022316,"end_time":"2025-01-31T23:39:26.898527","exception":false,"start_time":"2025-01-31T23:39:26.876211","status":"completed"},"tags":[],"id":"c2403f82"},"outputs":[],"source":["# list(test_df_pruned.columns)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":10854531,"sourceId":87997,"sourceType":"competition"}],"dockerImageVersionId":30839,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":962.99358,"end_time":"2025-01-31T23:39:30.518076","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-31T23:23:27.524496","version":"2.6.0"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}